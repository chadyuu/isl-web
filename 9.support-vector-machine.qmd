---
title: "Chapter 9: Support Vector Machines"
execute:
  cache: true
---

Here are three types of "Support Vector Machines."

-   maximal margin classifier: cannot be applied to most datasets.

-   support vector classifier: an extension of the maximal margin classifier

-   Support Vector Machine: a further extension of the support vector classifier

## Maximal Margin Classifier

In a p-dimensional space, a **hyperplane** is a flat affine subspace of dimension $p-1$, defined as:

![](images/paste-4C8FB5EB.png)

A heperplane can perfectly separate dataset into two classes, but such hyperplanes exist infinitely.

So we should choose the maximal margin hyperplane (optimal separating hyperplane) with the farthest minimum distance as margin.

![](images/paste-9A0FFD0A.png){width="368"}

The Maximal margin hyperplane is the solution to the optimizaiton problem:

![](images/paste-D9E20B6A.png)

The problem is that there are many cases with no separating hyperplane.

## Support Vector Classifiers (Soft Margin Classifier)

It allows some observations to be on the incorrect side of the margin or the hyperplane.

It is the solution to the optimization problem:

![](images/paste-7A55C68E.png)

where C is a non-negative tuning parameter, chosen via cross-validation.

$\epsilon_1,…,\epsilon_n$ are slack variables.

The smaller C, the lower bias and the higher variance. The larger C, the higher bias and the lower variance.

## Support Vector Machine

This support a non-linear decision boundary.

![](images/paste-D444CC12.png)

Some calculation, we find that the linear support vector classifier can be represented as

![where ![](images/paste-A5383EE3.png){width="164"}](images/paste-868149BA.png)

We replace the inner product with a generalized form:![](images/paste-892059C4.png){width="67"}, called **kernel**.

![](images/paste-1E1813C0.png)

For instance,

![](images/paste-214E9822.png){width="213"}

called a linear kernel, which gives the support vector classifier with a linear decision boundary.

![](images/paste-76549465.png){width="281"}

is known as a polynomial kernel of degree d, which gives a non-linear decision boundary when d \> 1.

Another popular choice is a radial kernel:

![$\gamma$ is a positive constant.](images/paste-9B7955FE.png)

## SVMs with More than Two Classes

### One-Versus-One (all-pairs) Classification

This constructs $\begin{pmatrix}k \\ 2\end{pmatrix}$ SVMs. The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these pairwise classifications.

### One-Versus-All classification

1.  Fit K SVMs, each time comparing one of the K classes to the remaining K-1 classes.
    1.  Estimate the parameters $\beta_{0k}, \beta_{1k},…,\beta_{pk}$ from fitting an SVM comparing the Kth class.
2.  classify the test observations by $\beta_{0k} + \beta_{1k} x_1 + … + \beta_{pk}x_p$ for each K, and pick the class with the largest value.

## Lab

### Support Vector Classifier

```{r}
set.seed (1)
x <- matrix(rnorm (20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
```

```{r}
dat <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~ ., data = dat , kernel = "linear", cost = 10, scale = FALSE)
```

```{r}
plot(svmfit , dat)
```

```{r}
summary(svmfit)
```

```{r}
svmfit <- svm(y ~ ., data = dat , kernel = "linear", cost = 0.1, scale = FALSE)
plot(svmfit , dat)
```

```{r}
set.seed (1)
tune.out <- tune(svm , y ~ ., data = dat , kernel = "linear", ranges = list(cost = c(0.001 , 0.01, 0.1, 1, 5, 10, 100)))
```

```{r}
summary(tune.out)
```

```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

```{r}
xtest <- matrix(rnorm (20 * 2), ncol = 2)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest , y = as.factor(ytest))
```

```{r}
ypred <- predict(bestmod , testdat)
table(predict = ypred , truth = testdat$y)
```

```{r}
svmfit <- svm(y ~ ., data = dat , kernel = "linear", cost = .01, scale = FALSE)
ypred <- predict(svmfit , testdat)
table(predict = ypred , truth = testdat$y)
```

```{r}
x[y == 1, ] <- x[y == 1, ] + 0.5 # more separte the two classes
plot(x, col = (y + 5) / 2, pch = 19)
```

```{r}
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat , kernel = "linear", cost = 1e5)
summary(svmfit)
```

```{r}
svmfit <- svm(y ~ ., data = dat , kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit , dat)
```

### SVM

```{r}
set.seed (1)
x <- matrix(rnorm (200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150) , rep(2, 50))
dat <- data.frame(x = x, y = as.factor(y))
```

```{r}
plot(x, col = y)
```

```{r}
train <- sample (200 , 100)
svmfit <- svm(y ~ ., data = dat[train , ], kernel = "radial", gamma = 1, cost = 1)
plot(svmfit , dat[train , ])
```

```{r}
summary(svmfit)
```

```{r}
svmfit <- svm(y ~ ., data = dat[train , ], kernel = "radial", gamma = 1, cost = 1e5)
plot(svmfit , dat[train , ])
```

```{r}
set.seed (1)
tune.out <- tune(svm , y ~ ., data = dat[train , ], kernel = "radial",
  ranges = list(
    cost = c(0.1 , 1, 10, 100, 1000) ,
    gamma = c(0.5, 1, 2, 3, 4)
  )
)
summary(tune.out)
```

\

```{r}
table(
  true = dat[-train , "y"],
  pred = predict(
    tune.out$best.model , newdata = dat[-train , ]
  )
)
```

### ROC Curves

```{r}
library(ROCR)
rocplot <- function(pred , truth , ...) {
  predob <- prediction(pred , truth)
  perf <- performance(predob , "tpr", "fpr")
  plot(perf , ...)
}
```

By increasing $\gamma$, we can produce a more flexible fit.

```{r}
par(mfrow = c(1, 2))

svmfit.opt <- svm(y ~ ., data = dat[train , ], kernel = "radial", gamma = 2, cost = 1,
decision.values = T)
fitted <- attributes(
  predict(svmfit.opt , dat[train , ], decision.values = TRUE)
)$decision.values
rocplot(-fitted, dat[train , "y"], main = "Training Data")

svmfit.flex <- svm(y ~ ., data = dat[train , ], kernel = "radial", gamma = 50, cost = 1,
decision.values = T)
fitted <- attributes(
  predict(svmfit.flex , dat[train , ], decision.values = T)
)$decision.values

rocplot(-fitted , dat[train , "y"], add = T, col = "red")
```

```{r}
fitted <- attributes(
predict(svmfit.opt , dat[-train , ], decision.values = T)
)$decision.values

rocplot(-fitted , dat[-train , "y"], main = "Test Data")
fitted <- attributes(
  predict(svmfit.flex , dat[-train , ], decision.values = T)
)$decision.values
rocplot(-fitted , dat[-train , "y"], add = T, col = "red")
```

### SVM with Multiple Classes

```{r}
set.seed (1)
x <- rbind(x, matrix(rnorm (50 * 2), ncol = 2))
y <- c(y, rep(0, 50))
x[y == 0, 2] <- x[y == 0, 2] + 2
dat <- data.frame(x = x, y = as.factor(y))
par(mfrow = c(1, 1))
plot(x, col = (y + 1))
```

The `svm` function performs multi-class classification using the one-versus-one approach.

```{r}
svmfit <- svm(y ~ ., data = dat , kernel = "radial", cost = 10, gamma = 1)
plot(svmfit , dat)
```

### E.g., Application to Gene Expression Data

```{r}
library(ISLR2)
names(Khan)
```

```{r}
table(Khan$ytrain)
```

```{r}
table(Khan$ytest)
```

```{r}
dat <- data.frame(
  x = Khan$xtrain,
  y = as.factor(Khan$ytrain)
)
out <- svm(y ~ ., data = dat , kernel = "linear", cost = 10)
summary(out)
```

```{r}
dat.te <- data.frame(
  x = Khan$xtest,
  y = as.factor(Khan$ytest))
pred.te <- predict(out , newdata = dat.te)
table(pred.te, dat.te$y)
```
