[
  {
    "objectID": "5.resampling.html",
    "href": "5.resampling.html",
    "title": "Chapter 5: Resampling Methods",
    "section": "",
    "text": "cross-validation\nbootstrap"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "This is a note for the book “An Introduction to Statistical Learning.”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "5.resampling.html#cross-validation",
    "href": "5.resampling.html#cross-validation",
    "title": "Chapter 5: Resampling Methods",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nThe Validation Set Approach\nSplit data into the following two sets\n\na training set\na validation set (hold-out set)\n\nWe can estimate the test error rate from the validation set error rate, which can be highly variable.\n\n\nLeave-One-Out Cross-Validation (LOOCV)\nLOOCV addresses the high variability of the validation set approach.\nWith least squares linear or polynomial regression, the cost of LOOCV is the same as that of a single model.\n\n\n\nk-Fold Cross-Validation\nIt randomly divides the set of observations into k groups.\n\nLOOCV is the special case of k-fold CV in which k is set to equal n.\n\nk-fold CV with k < n has a computational advantage to LOOCV.\nLOOCV reduces more bias than k-fold CV.\nLOOCV has higher variance than k-fold CV.\n\neach of n models is trained on an almost identical set of observations.\n\nk-fold CV often gives more accurate estimates of the test error rate than LOOCV.\n\nSo typically, k = 5 or k = 10 tends to yield more accurate test error rate estimates.\n\n\nCross-Validation on Classification Problems\n\n\\[Err_i = I(y_i \\neq \\hat{y_i})\\]"
  },
  {
    "objectID": "5.resampling.html#the-bootstrap",
    "href": "5.resampling.html#the-bootstrap",
    "title": "Chapter 5: Resampling Methods",
    "section": "The Bootstrap",
    "text": "The Bootstrap\nit is used to quantify the uncertainty associated with a given estimator.\nE.g., the bootstrap can be used to estimate the standard errors of the coefficients."
  },
  {
    "objectID": "5.resampling.html#lab",
    "href": "5.resampling.html#lab",
    "title": "Chapter 5: Resampling Methods",
    "section": "Lab",
    "text": "Lab\n\nThe Validation Set Approach\n\nlibrary(ISLR2)\nset.seed(1)\ntrain = sample(392, 196)\n\nSimple Linear Regression Model:\n\nlm.fit = lm(mpg ~ horsepower, data = Auto, subset = train)\n\n\nattach(Auto)\nmean((mpg - predict(lm.fit, Auto))[-train]^2) # the estimated test MSE\n\n[1] 23.26601\n\n\npolynomial regression models:\n\nlm.fit2 = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2) # the estimated test MSE\n\n[1] 18.71646\n\n\n\nlm.fit3 = lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2) # the estimated test MSE\n\n[1] 18.79401\n\n\nWe prefer the quadratic regression model based on the validation MSE.\n\n\nLOOCV\n\nglm.fit = glm(mpg ~ horsepower, data = Auto)\ncoef(glm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\n\nlibrary(boot)\ncv.err = cv.glm(Auto, glm.fit)\ncv.err$delta # [1]: the standard k-fold CV estimate, [2]: a bias-corrected version\n\n[1] 24.23151 24.23114\n\n\n\ncv.error = rep(0, 10)\nfor(i in 1:10){\n  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093\n\n\n\nplot(cv.error)\n\n\n\n\nWe see a sharp drop in the estimated test MSE between the linear and quadratic fits, while not from higher-order polynomials.\n\n\nk-Fold Cross Validation\n\nset.seed(17)\ncv.error.10 = rep(0, 10)\nfor (i in 1:10) {\n  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\nplot(cv.error.10)\n\n\n\n\nHigher-order polynomials than quadratic does not show improvement of test MSE.\n\n\nThe Bootstrap\n\nhead(Portfolio)\n\n           X          Y\n1 -0.8952509 -0.2349235\n2 -1.5624543 -0.8851760\n3 -0.4170899  0.2718880\n4  1.0443557 -0.7341975\n5 -0.3155684  0.8419834\n6 -1.7371238 -2.0371910\n\n\n\nalpha.fn = function(data, index){\n  X = data$X[index]\n  Y = data$Y[index]\n  (var(Y) - cov(X, Y) / (var(X) + var(Y) - 2 * cov(X, Y)))\n}\n\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.7792916\n\n\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\n[1] 0.9142495\n\n\n\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.7792916 -0.02651568   0.1723508\n\n\n\nboot.fn = function(data, index){\n  coef(lm(mpg ~ horsepower, data= data, subset = index))\n}\n\n\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\n\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0549915227 0.841925746\nt2* -0.1578447 -0.0006210818 0.007348956\n\n\n\nsummary(lm(mpg ~ horsepower , data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\n\nboot.fn2 = function(data, index){\n  coef(lm(mpg ~ horsepower + I(horsepower^2), data= data, subset = index))\n}\n\n\nset.seed(1)\nboot(Auto, boot.fn2, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn2, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\n\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))\n\n\nCall:\nlm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     56.9000997  1.8004268   31.60   <2e-16 ***\nhorsepower      -0.4661896  0.0311246  -14.98   <2e-16 ***\nI(horsepower^2)  0.0012305  0.0001221   10.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "6.model-selection.html",
    "href": "6.model-selection.html",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "",
    "text": "There are three ways to select model."
  },
  {
    "objectID": "6.model-selection.html#subset-selection",
    "href": "6.model-selection.html#subset-selection",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Subset Selection",
    "text": "Subset Selection\nThe exhaustive search entails computational limitations; the number of possible models grows rapidly as p increases.\nSo we take advantage of the following stepwise selection.\n\nForward Stepwise Selection\nBackward Stepwise Selection\nHybrid Approaches of forward and backward\n\nWe determine the best model based on the following statistics.\n\nThe indirect estimates of test error by making an adjustment to the training error\n\n\\(C_p\\)\nBIC\nadjusted \\(R^2\\)\n\nThe direct estimate of the test error\n\ncross-validated prediction error\n\n\nWhile we preferred the indirect estimate in the past with limitation of computational resources, the cross-validation prediction error obtains popularity today.\n\nStepwise Selection\n\\[\nC_p = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2)\n\\]\n\\[\nAIC = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2)\n\\]\n\\[\nBIC = \\frac{1}{n}(RSS + log(n) \\hat{\\sigma}^2)\n\\]\n\\(d \\hat{\\sigma}^2\\) is a penalty for the number of predictors.\nFor the Gaussian model (with variance \\(\\sigma_\\epsilon ^2\\) = \\(\\hat{\\sigma}_\\epsilon ^2\\)), the AIC is equivalent to \\(C_p\\).\n\\[\nAdjusted R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}\n\\]\n\\(d\\) is the number of variables."
  },
  {
    "objectID": "6.model-selection.html#shrinkage-model",
    "href": "6.model-selection.html#shrinkage-model",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Shrinkage Model",
    "text": "Shrinkage Model\n\nRidge Regression: \\(l_2\\) penalty\nLasso: \\(l_1\\) penalty\n\n\nRidge Regression\nInstead of RSS,\n\nthe ridge regression estimates \\(\\hat{\\beta^R}\\) that minimize the following function.\n\n\\(\\lambda \\sum\\beta_j^2\\) is called a shrinkage penalty.\nNote that we do not impose penalty on the intercept.\nIt is best to apply ridge regression after standardizing the predictors since the \\(X_j \\hat{\\beta}^R_{j,\\lambda}\\) is depend on \\(\\lambda\\) and the scaling of the other predictors.\nAs \\(\\lambda\\) increases, the ridge regression leads to a significant reduction in the variance of the predictions at the expense of a slight increase in bias under some value of \\(\\lambda\\). Therefore, the shrinkage tends to generate a better fit for the test dataset than the least squares approach.\nThe disadvantage of the ridge regression is to always include all p predictors.\n\n\nThe Lasso\nThe lasso coefficients minimize the following quantity.\n\nThe \\(l_1\\) penalty forces some of the coefficients to be exactly equal to zero.\nNeither ridge regression nor the lasso will universally dominate the other. The lasso outperform the ridge regression in case that a small number of the predictors are significant. However, we cannot know their significance, so cross-validation can be used to determine which approach is better.\nThe ridge regression shrinks every dimension by the same proportion, whereas the lasso shrinks all coefficients toward zero by a similar amount.\n\n\n\nHow to determine \\(\\lambda\\)\nFind \\(\\lambda\\) from the cross-validation error for each \\(\\lambda\\)."
  },
  {
    "objectID": "6.model-selection.html#dimension-reduction-methods",
    "href": "6.model-selection.html#dimension-reduction-methods",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nPrincipal Components Regression (PCR)\nAn unsupervised method, which is not always the best to predict.\n\nRecommend to standardize all the predictors since the high-variance variables tend to play a larger role .\nConstruct the first M principal components, \\(Z_1, Z_2, …, Z_M\\).\n\nThe first principal component direction is that along which the observations vary the most.\nThe second principal component direction must be perpendicular or orthogonal to the first principal component direction. Under this constraint, it also must have largest variance.\n\nFit the model with the M principal components, which can avoid overfitting than the model with all p variables.\nThe value of M is typically chosen by cross-validation.\n\n\n\nPartial least squares (PLS)\nA supervised method.\n\nStandardize the p predictors.\nConstruct the first M principal components, \\(Z_1, Z_2, …, Z_M\\).\n\nCompute the first direction \\(Z_1\\) by setting each coefficient of X equal to those from the simple linear regression.\nTake residuals between actual values and \\(Z_1\\).\nCompute the second direction \\(Z_2\\) for the residuals with least squares.\n\n\nWhile PLS can reduce bias, it has the potential to increase variance. So it often performs no better than ridge regression or PCR."
  },
  {
    "objectID": "6.model-selection.html#regression-in-high-dimensions",
    "href": "6.model-selection.html#regression-in-high-dimensions",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Regression in High Dimensions",
    "text": "Regression in High Dimensions\nThe model with more predictors than the number of observations fits data exactly, which often leads to overfitting.\nHere are the workarounds.\n\nforward stepwise selection\nridge regression\nthe lasso\nprincipal components regression\n\nAlso, pay attention to the interpretation. If the variables entail multicollinearity, the values of coefficients are uninterpretable."
  },
  {
    "objectID": "6.model-selection.html#lab",
    "href": "6.model-selection.html#lab",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Lab",
    "text": "Lab\n\nlibrary(ISLR2)\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\n\nHitters = na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\n\n\nBest subset selection\n\nlibrary(leaps)\nregfit.full = regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\n\nregfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nreg.summary = summary(regfit.full)\n\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\npar(mfrow = c(2,2))\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\n\nwhich.max(reg.summary$adjr2)\n\n[1] 11\n\n\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\npoints(11, reg.summary$adjr2[11], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\nForward and Backward Stepwise Selection\n\nregfit.fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\n\nregfit.bwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\n\n\nCross-validation\n\nset.seed(1)\ntrain = sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)\ntest = (!train)\n\n\nregfit.best = regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)\n\n\ntest.mat = model.matrix(Salary ~ ., data = Hitters[test, ])\n\n\nval.errors = rep(NA, 19)\nfor(i in 1:19){\n  coefi = coef(regfit.best, id = i)\n  pred = test.mat[, names(coefi)] %*% coefi\n  val.errors[i] = mean((Hitters$Salary[test] - pred)^2)\n}\n\n\ncoef(regfit.best, which.min(val.errors))\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\n\npredict.regsubsets = function(object, newdata, id){\n  form = as.formula(object$call[[2]])\n  mat = model.matrix(form, newdata)\n  coefi = coef(object, id = id)\n  xvars = names(coefi)\n  mat[, xvars] %*% coefi\n}\n\n\nregfit.best = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\n\nk = 10\nn = nrow(Hitters)\nset.seed(1)\nfolds = sample(rep(1:k, length = n))\ncv.errors = matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))\n\n\nfor(j in 1:k){\n  best.fit = regsubsets(Salary ~ ., data = Hitters[folds != j,], nvmax = 19)\n  for (i in 1:19){\n    pred = predict(best.fit, Hitters[folds ==j, ], id = i)\n    cv.errors[j, i] = mean((Hitters$Salary[folds == j] - pred)^2)\n  }\n}\n\n\nmean.cv.errors = apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\n\n\npar(mfrow = c(1,1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\nWe found a 10-variable model is the best through cross-validation.\n\n\nRidge Regression\n\nx = model.matrix(Salary ~ ., Hitters)[, -1]\ny = Hitters$Salary\n\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-4\n\ngrid = 10^seq(10, -2, length = 100)\nridge.mod = glmnet(x, y, alpha = 0, lambda = grid)\n\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\n\n\npredict(ridge.mod, s = 50, type = \"coefficients\") # lambda = 50\n\n20 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  4.876610e+01\nAtBat       -3.580999e-01\nHits         1.969359e+00\nHmRun       -1.278248e+00\nRuns         1.145892e+00\nRBI          8.038292e-01\nWalks        2.716186e+00\nYears       -6.218319e+00\nCAtBat       5.447837e-03\nCHits        1.064895e-01\nCHmRun       6.244860e-01\nCRuns        2.214985e-01\nCRBI         2.186914e-01\nCWalks      -1.500245e-01\nLeagueN      4.592589e+01\nDivisionW   -1.182011e+02\nPutOuts      2.502322e-01\nAssists      1.215665e-01\nErrors      -3.278600e+00\nNewLeagueN  -9.496680e+00\n\n\n\nset.seed(1)\ntrain = sample(1:nrow(x), nrow(x) / 2)\ntest = (-train)\ny.test = y[test]\n\n\nridge.mod = glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)\nridge.pred = predict(ridge.mod, s = 4, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\n\nset.seed(1)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 0)\nplot(cv.out)\n\n\n\n\n\nbestlam = cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\n\nridge.pred = predict(ridge.mod, s = bestlam, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\n\nout = glmnet(x, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\n\n\nThe Lasso\n\nlasso.mod = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\n\nset.seed(1)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\n\n\nbestlam = cv.out$lambda.min\nlasso.pred = predict(lasso.mod, s = bestlam, newx = x[test, ])\nmean((lasso.pred -y.test)^2)\n\n[1] 143673.6\n\n\n\nout = glmnet(x, y, alpha = 1, lambda = grid)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\n\n\n\nPCR regression\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nset.seed(2)\npcr.fit = pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = \"CV\") # ten-fold cross-validation\n\n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    351.9    353.2    355.0    352.8    348.4    343.6\nadjCV          452    351.6    352.7    354.4    352.1    347.6    342.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       345.5    347.7    349.6     351.4     352.1     353.5     358.2\nadjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.7     349.4     339.9     341.6     339.2     339.6\nadjCV     348.0     347.7     338.2     339.7     337.2     337.6\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\n\nvalidationplot(pcr.fit, val.type = \"MSEP\") # cross-validation MSE to be plotted\n\n\n\n\n\nset.seed(1)\npcr.fit = pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\n\npcr.pred = predict(pcr.fit, x[test, ], ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142811.8\n\n\n\npcr.fit = pcr(y ~ x, scale = TRUE, ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90\n\n\n\n\nPartial Least Squares\n\nset.seed(1)\npls.fit = plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nsummary(pls.fit)\n\nData:   X dimension: 131 19 \n    Y dimension: 131 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           428.3    325.5    329.9    328.8    339.0    338.9    340.1\nadjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       339.0    347.1    346.4     343.4     341.5     345.4     356.4\nadjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        348.4     349.1     350.0     344.2     344.5     345.0\nadjCV     344.2     345.0     345.9     340.4     340.6     341.1\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71\nSalary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         93.17     96.05     97.08     97.61     97.97     98.70     99.12\nSalary    55.95     56.12     56.47     56.68     57.37     57.76     58.08\n        16 comps  17 comps  18 comps  19 comps\nX          99.61     99.70     99.95    100.00\nSalary     58.17     58.49     58.56     58.62\n\n\n\nvalidationplot(pls.fit, val.type = \"MSEP\")\n\n\n\n\n\npls.pred = predict(pls.fit, x[test, ], ncomp = 1)\nmean((pls.pred - y.test)^2)\n\n[1] 151995.3\n\n\n\npls.fit = plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 1)\nsummary(pls.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: kernelpls\nNumber of components considered: 1\nTRAINING: % variance explained\n        1 comps\nX         38.08\nSalary    43.05"
  },
  {
    "objectID": "8.tree-based.html",
    "href": "8.tree-based.html",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "",
    "text": "The tree-based methods are useful for interpretation but not competitive in prediction."
  },
  {
    "objectID": "8.tree-based.html#the-basics-of-decision-trees",
    "href": "8.tree-based.html#the-basics-of-decision-trees",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "The basics of Decision Trees",
    "text": "The basics of Decision Trees\n\nRegression Trees\n\nDivide the predictor space into J distinct and non-overlapping regions, \\(R_1, R_2, ..., R_j\\).\nReturn the mean of the response values for the training observations in \\(R_j\\).;\n\nThe goal is to find boxes \\(R_1, R_2, ..., R_j\\) that minimize the following RSS.\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j}(y_i - \\hat{y}_{R_J})^2\n\\]"
  },
  {
    "objectID": "8.tree-based.html#regression-trees",
    "href": "8.tree-based.html#regression-trees",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\n\nDivide the predictor space into J distinct and non-overlapping regions, \\(R_1, R_2, ..., R_j\\).\nReturn the mean of the response values for the training observations in \\(R_j\\).;\n\nThe goal is to find boxes \\(R_1, R_2, ..., R_j\\) that minimize the following RSS.\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j}(y_i - \\hat{y}_{R_J})^2\n\\]\nIt is computationally infeasible to consider every possible partition. Therefore, we take a top-down, greedy algorithm, recursive binary splitting.\nGiven\n\n\n\nseek the value of j and sn that minimize the following equation.\n\n\n\nNext, we split one of these two regions in the same way.\n\nTree Pruning\nHowever, this method tends to cause overfitting. So we adopt tree pruning that incorporates splitting as long as the decrease in the RSS exceeds some threshold.\nTo avoid to discard a split that leas to a large reduction in RSS later on, grow a very large tree \\(T_0\\), and then prune it back in order. Here, since estimating the cross-validation error for every possible subtree would be too cumbersome, we select a small set of subtrees for consideration, called cost complexity pruning or weakest link pruning.\nFind a subtree \\(T \\in T_0\\) that minimizes the following function.\n\n\\(|T|\\) is the number of terminal nodes of the tree T. \\(\\alpha |T|\\) is a penalty for a larger tree.\nWe determine \\(\\alpha\\) by cross-validation."
  },
  {
    "objectID": "8.tree-based.html#classification-trees",
    "href": "8.tree-based.html#classification-trees",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Classification Trees",
    "text": "Classification Trees\nWe use one of the following three criteria instead of RSS for classification trees.\n\nClassification error rate\n\n\n\nThe Gini index\n\n\\(\\hat{P}_{mk}\\) represents the proportion of training observations in the mth region that are from the kth class.\nThe small Gini index indicates a node contains predominantly observations from a single class.\n\n\nEntropy\n\nHere is the graph of \\(-xlog(x)\\).\n\nThe Gini index and entropy are quite similar numerically.\nIf prediction accuracy is the goal, the classification error rate is preferable."
  },
  {
    "objectID": "8.tree-based.html#bagging",
    "href": "8.tree-based.html#bagging",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Bagging",
    "text": "Bagging\nBagging is the method with the following steps.\n\nBootstrap by taking repeated samples from the training dataset.\nTrain our method on the bth bootstrapped training set.\nAverage all the predictions.\n\nAveraging a set of observations reduces variance. Given \\(Z_1, …, Z_n\\), each with variance \\(\\sigma^2\\), the variance of \\(\\bar{Z}\\) is \\(\\sigma^2/n\\).\nOut-of-Bag error estimation is a very straightforward way to estimate test error without the need to perform cross-validation.\n\nPredict the response for Out-of-bag (OOB) observations not used to fit a given bagged tree.\nAverage the predictions and compute overall OOB MSE or classification error, which is valid estimates of the test error.\n\nBagging improves prediction accuracy at the expense of interpretability. Also, in bagging, many of the bagged trees tend to be highly correlated, leading a less reduction in variance than averaging many uncorrelated quantities.\nBagging can get caught in local optima."
  },
  {
    "objectID": "8.tree-based.html#random-forests",
    "href": "8.tree-based.html#random-forests",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Random Forests",
    "text": "Random Forests\nRandom forests forces each split to consider only a random subset of the predictors. (Typically, we choose \\(m = \\sqrt{p}\\).) This make the average of the resulting trees less variable and more reliable."
  },
  {
    "objectID": "8.tree-based.html#boosting",
    "href": "8.tree-based.html#boosting",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Boosting",
    "text": "Boosting\nIn boosting, the trees are grown sequentially.\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\).\nGiven the current model, fit a decision tree to the residuals from the model as \\(\\hat{f}^b(x)\\) with d splits.\nAdd this new decision tree into the fitted function to update the residuals. \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\).\nUpdate the residuals. \\(r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\).\nOutput the boosted model. \\(\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x)\\).\n\nBoosting can overfit if B is too large.\nTypical values of \\(\\lambda\\) is 0.01 or 0.001.\nThe number of splits often \\(d = 1\\)."
  },
  {
    "objectID": "8.tree-based.html#bayesian-additive-regression-trees-bart",
    "href": "8.tree-based.html#bayesian-additive-regression-trees-bart",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Bayesian Additive Regression Trees (BART)",
    "text": "Bayesian Additive Regression Trees (BART)\nDefine three variables.\n\nK: the number of trees\nB: the number of iterations\nL: the number of burn-in iterations\n\nHere is the algorithm:\n\n\\(\\hat{f}_1^1 (x) = \\hat{f}_2^1 (x) = ... = \\hat{f}_K^1 (x) = \\frac{1}{nK} \\sum_{i=1}^{n} y_i\\)\n\\(\\hat{f}^1(x) = \\sum_{k=1}^{K} \\hat{f}_k^1(x) = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\)\nFor \\(b = 2,…,B\\):\n\nFor \\(k = 1,2,…,K\\):\n\nFor \\(i = 1,…,n\\), compute the current partial residual:\n\n\\(r_i = y_i - \\sum_{k' < k} \\hat{f}_{k'}^b (x_i) - \\sum_{k' > k} \\hat{f}_{k'}^{b-1}(x_i)\\)\nFit a new tree, \\(\\hat{f}_k^b(x)\\) to \\(r_i\\).\n\n\nCompute \\(\\hat{f}^b(x) = \\sum_{k=1}{K} \\hat{f}_k^b (x)\\)\n\nCompute the mean after L burn-in samples:\n\n\\(\\hat{f}(x) = \\frac{1}{B-L} \\sum_{b=L+1}{B} \\hat{f}^b(x)\\)\n\n\nWe typically throw away the first few models since they tend not to provide very good results.\nWe typically choose large values for B and K, and a moderate value for L: e.g., K=200, B=1000, L=100."
  },
  {
    "objectID": "8.tree-based.html#lab",
    "href": "8.tree-based.html#lab",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Lab",
    "text": "Lab\n\nFitting Classification Trees\n\nlibrary(tree)\nlibrary(ISLR2)\nattach(Carseats)\nHigh = factor(ifelse(Sales <= 8, \"No\", \"Yes\"))\n\n\nCarseats = data.frame(Carseats, High)\n\n\ntree.carseats = tree(High ~ . - Sales, Carseats)\n\n\nsummary(tree.carseats)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n\n\n\nplot(tree.carseats)\ntext(tree.carseats, pretty=0)\n\n\n\n\n\ntree.carseats\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 541.500 No ( 0.59000 0.41000 )  \n    2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n      4) Price < 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n        8) Income < 57 10  12.220 No ( 0.70000 0.30000 )  \n         16) CompPrice < 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n         17) CompPrice > 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n        9) Income > 57 36  35.470 Yes ( 0.19444 0.80556 )  \n         18) Population < 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n         19) Population > 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n      5) Price > 92.5 269 299.800 No ( 0.75465 0.24535 )  \n       10) Advertising < 13.5 224 213.200 No ( 0.81696 0.18304 )  \n         20) CompPrice < 124.5 96  44.890 No ( 0.93750 0.06250 )  \n           40) Price < 106.5 38  33.150 No ( 0.84211 0.15789 )  \n             80) Population < 177 12  16.300 No ( 0.58333 0.41667 )  \n              160) Income < 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n              161) Income > 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n             81) Population > 177 26   8.477 No ( 0.96154 0.03846 ) *\n           41) Price > 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n         21) CompPrice > 124.5 128 150.200 No ( 0.72656 0.27344 )  \n           42) Price < 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n             84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n             85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n              170) Price < 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n              171) Price > 109.5 24  32.600 No ( 0.58333 0.41667 )  \n                342) Age < 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n                343) Age > 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n           43) Price > 122.5 77  55.540 No ( 0.88312 0.11688 )  \n             86) CompPrice < 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n             87) CompPrice > 147.5 19  25.010 No ( 0.63158 0.36842 )  \n              174) Price < 147 12  16.300 Yes ( 0.41667 0.58333 )  \n                348) CompPrice < 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n                349) CompPrice > 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n              175) Price > 147 7   0.000 No ( 1.00000 0.00000 ) *\n       11) Advertising > 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n         22) Age < 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n           44) CompPrice < 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n             88) Income < 100 9  12.370 No ( 0.55556 0.44444 ) *\n             89) Income > 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n           45) CompPrice > 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Age > 54.5 20  22.490 No ( 0.75000 0.25000 )  \n           46) CompPrice < 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n           47) CompPrice > 122.5 10  13.860 No ( 0.50000 0.50000 )  \n             94) Price < 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n             95) Price > 125 5   0.000 No ( 1.00000 0.00000 ) *\n    3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n      6) Price < 135 68  49.260 Yes ( 0.11765 0.88235 )  \n       12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n         24) Price < 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price > 109 9  11.460 No ( 0.66667 0.33333 ) *\n       13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n      7) Price > 135 17  22.070 No ( 0.64706 0.35294 )  \n       14) Income < 46 6   0.000 No ( 1.00000 0.00000 ) *\n       15) Income > 46 11  15.160 Yes ( 0.45455 0.54545 ) *\n\n\n\nset.seed(2)\ntrain = sample(1:nrow(Carseats), 200)\nCarseats.test = Carseats[-train, ]\nHigh.test = High[-train]\ntree.carseats = tree(High ~ . - Sales, Carseats, subset = train)\ntree.pred = predict(tree.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\n\n\n(104 + 50) / 200\n\n[1] 0.77\n\n\nThe function cv.tree() performs cross-validation to determine the optimal level of tree complexity. FUN=prune.misclass indicates that we want the classification error rate to guide the cross-validation and pruning process. (The default is deviance.)\n\nset.seed(7)\ncv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)\ncv.carseats\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nsize: the number of terminal nodes of each tree\ndev: the error rate (the number of cross-validation error)\nk: the value of the cost-complexity parameter (\\(\\alpha\\))\n\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n\n\n\n\nThe size of 9 has the lowest dev.\n\nprune.carseats = prune.misclass(tree.carseats, best = 9) # obtain the nine-node tree\nplot(prune.carseats) \ntext(prune.carseats, pretty = 0)\n\n\n\n\n\ntree.pred = predict(prune.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred No Yes\n      No  97  25\n      Yes 20  58\n\n\n\n(97 + 58) / 200\n\n[1] 0.775\n\n\nThe pruned tree produced a more interpretable tree and also slightly improved classification accuracy.\nA larger pruned tree has lower classification accuracy:\n\nprune.carseats = prune.misclass(tree.carseats, best = 14) # obtain the 14-node tree\nplot(prune.carseats) \ntext(prune.carseats, pretty = 0)\n\n\n\n\n\ntree.pred = predict(prune.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred  No Yes\n      No  102  31\n      Yes  15  52\n\n\n\n(102 + 52) / 200\n\n[1] 0.77\n\n\n\n\nFitting Regression Trees\n\nset.seed (1)\ntrain <- sample (1: nrow(Boston), nrow(Boston) / 2)\ntree.boston <- tree(medv ~ ., Boston , subset = train)\nsummary(tree.boston)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n\n\n\nplot(tree.boston)\ntext(tree.boston , pretty = 0)\n\n\n\n\n\ncv.boston <- cv.tree(tree.boston)\nplot(cv.boston$size , cv.boston$dev, type = \"b\")\n\n\n\n\n\nprune.boston <- prune.tree(tree.boston , best = 5)\nplot(prune.boston)\ntext(prune.boston , pretty = 0)\n\n\n\n\n\nyhat <- predict(tree.boston , newdata = Boston[-train , ])\nboston.test <- Boston[-train, \"medv\"]\nplot(yhat , boston.test)\nabline (0, 1)\n\n\n\n\n\nmean (( yhat - boston.test)^2)\n\n[1] 35.28688\n\n\n\nrpart\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ntr1 = rpart(medv ~ ., data = Boston)\npar(mfrow=c(1,2))\nplot(tr1)\nrpart.plot(tr1)\n\n\n\n\ncross-validation for pruning.\nCP: scaled \\(\\alpha\\).\nInstead of \\(RSS(T) + \\alpha |T|\\), rpart uses \\(\\frac{RSS(T)}{RSS(root)} + CP \\cdot |T|\\).\n\nprintcp(tr1)\n\n\nRegression tree:\nrpart(formula = medv ~ ., data = Boston)\n\nVariables actually used in tree construction:\n[1] crim  dis   lstat rm   \n\nRoot node error: 42716/506 = 84.42\n\nn= 506 \n\n        CP nsplit rel error  xerror     xstd\n1 0.452744      0   1.00000 1.00324 0.082971\n2 0.171172      1   0.54726 0.64125 0.057921\n3 0.071658      2   0.37608 0.43009 0.047418\n4 0.036164      3   0.30443 0.33278 0.041968\n5 0.033369      4   0.26826 0.32456 0.042045\n6 0.026613      5   0.23489 0.31425 0.041927\n7 0.015851      6   0.20828 0.28420 0.039942\n8 0.010000      7   0.19243 0.27750 0.040652\n\n\n\ntr1$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.45274420      0 1.0000000 1.0032418 0.08297088\n2 0.17117244      1 0.5472558 0.6412460 0.05792114\n3 0.07165784      2 0.3760834 0.4300911 0.04741826\n4 0.03616428      3 0.3044255 0.3327752 0.04196756\n5 0.03336923      4 0.2682612 0.3245644 0.04204536\n6 0.02661300      5 0.2348920 0.3142473 0.04192687\n7 0.01585116      6 0.2082790 0.2841993 0.03994168\n8 0.01000000      7 0.1924279 0.2774985 0.04065244\n\n\n\ncbind(tr1$cptable[, 1], c(-diff(tr1$cptable[, 3]), 0))\n\n        [,1]       [,2]\n1 0.45274420 0.45274420\n2 0.17117244 0.17117244\n3 0.07165784 0.07165784\n4 0.03616428 0.03616428\n5 0.03336923 0.03336923\n6 0.02661300 0.02661300\n7 0.01585116 0.01585116\n8 0.01000000 0.00000000\n\n\n\nprune(tr1, cp=0.3) # only 1 split\n\nn= 506 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 506 42716.300 22.53281  \n  2) rm< 6.941 430 17317.320 19.93372 *\n  3) rm>=6.941 76  6059.419 37.23816 *\n\n\n\nprune(tr1, cp=0.1) # 2 splits\n\nn= 506 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 506 42716.300 22.53281  \n  2) rm< 6.941 430 17317.320 19.93372  \n    4) lstat>=14.4 175  3373.251 14.95600 *\n    5) lstat< 14.4 255  6632.217 23.34980 *\n  3) rm>=6.941 76  6059.419 37.23816 *\n\n\n\nplotcp(tr1)\n\n\n\n\n\n\n\nBagging and Random Forests\n\nlibrary(randomForest)\nset.seed (1)\nbag.boston <- randomForest(medv ~ ., data = Boston, subset = train , mtry = 12, importance = TRUE)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n\n\nmtry indicates the number of predictors to be considered for each split.\n\nyhat.bag <- predict(bag.boston , newdata = Boston[-train , ])\nplot(yhat.bag , boston.test)\nabline (0, 1)\n\n\n\n\n\nmean (( yhat.bag - boston.test)^2)\n\n[1] 23.41916\n\n\n\nbag.boston <- randomForest(medv ~ ., data = Boston ,\nsubset = train , mtry = 12, ntree = 25)\nyhat.bag <- predict(bag.boston , newdata = Boston[-train , ])\nmean (( yhat.bag - boston.test)^2)\n\n[1] 25.75055\n\n\nntree indicates the number of trees.\nTo conduct random forest, we set mtry less than the number of variables.\n\nset.seed (1)\nrf.boston <- randomForest(medv ~ ., data = Boston,\n                          subset = train , mtry = 6, importance = TRUE)\nyhat.rf <- predict(rf.boston, newdata = Boston[-train , ])\nmean (( yhat.rf - boston.test)^2)\n\n[1] 20.06644\n\n\nThe prediction and error rate returned by randomForest are calculated based on OOB.\n\nimportance(rf.boston)\n\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n\n\n\nvarImpPlot(rf.boston)\n\n\n\n\n%IncMSE indicates the increase in MSE for regression / error rate for classification, when a given variable is not available (or permuted). In this case, lstat and rm are by far the two more important variables.\n\n\nBoosting\n\nlibrary(gbm)\nset.seed (1)\nboost.boston <- gbm(medv ~ ., data = Boston[train , ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4)\n\nWe set distribution as \"gaussian\" for a regression problem and \"bernoulli\" for a binary classification problem.\ninteraction.depth limits the depth of each tree.\n\nsummary(boost.boston)\n\n\n\n\n            var     rel.inf\nrm           rm 44.48249588\nlstat     lstat 32.70281223\ncrim       crim  4.85109954\ndis         dis  4.48693083\nnox         nox  3.75222394\nage         age  3.19769210\nptratio ptratio  2.81354826\ntax         tax  1.54417603\nindus     indus  1.03384666\nrad         rad  0.87625748\nzn           zn  0.16220479\nchas       chas  0.09671228\n\n\nBelow show partial dependence plots, illustrating the marginal effect of the selected variables on the response after integrating out the other variables.\n\npar(mfrow = c(1,2))\nplot(boost.boston , i = \"rm\")\n\n\n\nplot(boost.boston , i = \"lstat\")\n\n\n\n\n\nyhat.boost <- predict(boost.boston,\n                      newdata = Boston[-train , ], n.trees = 5000)\nmean (( yhat.boost - boston.test)^2)\n\n[1] 18.39057\n\n\n\nboost.boston <- gbm(medv ~ ., data = Boston[train , ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4, shrinkage = 0.2, verbose = F)\nyhat.boost <- predict(boost.boston ,\n                      newdata = Boston[-train , ], n.trees = 5000)\nmean (( yhat.boost - boston.test)^2)\n\n[1] 16.54778\n\n\nshrinkage is the value of the shrinkage parameter \\(\\lambda\\) with 0.001 as default. In this case, \\(\\lambda=0.2\\) produces a lower test MSE.\n\ngbm.perf(boost.boston)\n\nOOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds>1 when calling gbm usually results in improved predictive performance.\n\n\n\n\n\n[1] 100\nattr(,\"smoother\")\nCall:\nloess(formula = object$oobag.improve ~ x, enp.target = min(max(4, \n    length(x)/10), 50))\n\nNumber of Observations: 5000 \nEquivalent Number of Parameters: 39.99 \nResidual Standard Error: 0.3387 \n\n\n\n\nBayesian Additive Regression Trees\n\nlibrary(BART)\nx <- Boston[, 1:12]\ny <- Boston[, \"medv\"]\nxtrain <- x[train, ]\nytrain <- y[train]\nxtest <- x[-train, ]\nytest <- y[-train]\nset.seed (1)\nbartfit <- gbart(xtrain , ytrain , x.test = xtest)\n\n*****Calling gbart: type=1\n*****Data:\ndata:n,p,np: 253, 12, 253\ny1,yn: 0.213439, -5.486561\nx1,x[n*p]: 0.109590, 20.080000\nxp1,xp[np*p]: 0.027310, 7.880000\n*****Number of Trees: 200\n*****Number of Cut Points: 100 ... 100\n*****burn,nd,thin: 100,1000,1\n*****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.795495,3,3.71636,21.7866\n*****sigma: 4.367914\n*****w (weights): 1.000000 ... 1.000000\n*****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,12,0\n*****printevery: 100\n\nMCMC\ndone 0 (out of 1100)\ndone 100 (out of 1100)\ndone 200 (out of 1100)\ndone 300 (out of 1100)\ndone 400 (out of 1100)\ndone 500 (out of 1100)\ndone 600 (out of 1100)\ndone 700 (out of 1100)\ndone 800 (out of 1100)\ndone 900 (out of 1100)\ndone 1000 (out of 1100)\ntime: 3s\ntrcnt,tecnt: 1000,1000\n\n\n\nyhat.bart <- bartfit$yhat.test.mean\nmean (( ytest - yhat.bart)^2)\n\n[1] 15.94718\n\n\nWe can check how many times each variable appeared in the collection of trees.\n\nord <- order(bartfit$varcount.mean , decreasing = T)\nbartfit$varcount.mean[ord]\n\n    nox   lstat     tax     rad      rm   indus    chas ptratio     age      zn \n 22.952  21.329  21.250  20.781  19.890  19.825  19.051  18.976  18.274  15.952 \n    dis    crim \n 14.457  11.007"
  },
  {
    "objectID": "8.tree-based.html#boosting-gradient-boosting-machine-gbm",
    "href": "8.tree-based.html#boosting-gradient-boosting-machine-gbm",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Boosting (Gradient Boosting Machine: GBM)",
    "text": "Boosting (Gradient Boosting Machine: GBM)\nIn boosting, the trees are grown sequentially.\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\).\nGiven the current model, fit a decision tree to the residuals from the model as \\(\\hat{f}^b(x)\\) with d splits.\nAdd this new decision tree into the fitted function to update the residuals. \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\).\nUpdate the residuals. \\(r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\).\nOutput the boosted model. \\(\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x)\\).\n\nBoosting can overfit if B is too large.\nTypical values of \\(\\lambda\\) is 0.01 or 0.001.\nThe number of splits often \\(d = 1\\).\n\nRandom Forest vs. Boosting\nRandom Forest has less number of tuning parameters. With proper tuning, boosting can perform better than random forest."
  },
  {
    "objectID": "7.beyond-linearity.html",
    "href": "7.beyond-linearity.html",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "",
    "text": "Polynomial regression\nStep functions\nRegression splines\nSmoothing splines\nLocal regression\nGeneralized additive models"
  },
  {
    "objectID": "7.beyond-linearity.html#basis-functions",
    "href": "7.beyond-linearity.html#basis-functions",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Basis Functions",
    "text": "Basis Functions\nPolynomial and piecewise-constant regression models are in fact special cases of a basis function approach, a family of functions or transformations that can be applied to a variable.\n\nHence, we can use least squares."
  },
  {
    "objectID": "7.beyond-linearity.html#polynomial-regression",
    "href": "7.beyond-linearity.html#polynomial-regression",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression"
  },
  {
    "objectID": "7.beyond-linearity.html#step-functions",
    "href": "7.beyond-linearity.html#step-functions",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Step Functions",
    "text": "Step Functions\nBreak the range of X into bins to convert a continuous variable into an ordered categorical variable."
  },
  {
    "objectID": "7.beyond-linearity.html#regression-splines",
    "href": "7.beyond-linearity.html#regression-splines",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nPiecewise Polynomials\n\nThe problem is the function is discontinuous around knots.\n\n\nConstraints and Splines\nConstraints:\n\nthe fitted curve must be continuous.\nBoth the first and second derivatives of the piecewise polynomials are continuous at knots ( except a linear spline).\n\n\n\nThe Spline Basis Representation\n\nE.g., for a cubic polynomial, add one truncated power basis function per knot as follows.\n\nwhere \\(\\xi\\) is the knot.\nThen, the first and second derivatives at each of the knots will remain continuous.\nThe number of predictors is $3+K$, i.e., \\(X, X^2, X^3, h(X,\\xi_1), h(X,\\xi+2),...,h(X,\\xi_K)\\).\nWith the intercept in addition to these predictors, the number of coefficients is \\(K+4\\), i.e., \\(K+4\\) degrees of freedom.\nSplines can have high variance at the outer range of the predictors.\n\n\nChoosing the Number and Locations of the Knots\nIn practice, it is common to place knots in a uniform fashion.\nTo determine the number of knots or degrees of freedom, cross-validation works well.\n\n\nComparison to Polynomial Regression\nRegression splines often give superior results to polynomial regression. Regression splines, which increase the number of knots but keep the degree fixed, produce more stable estimates."
  },
  {
    "objectID": "7.beyond-linearity.html#smoothing-splines",
    "href": "7.beyond-linearity.html#smoothing-splines",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\nThe function g that minimizes the following value is called smoothing splines, which makes RSS small but also the regression line smooth.\n\nwhere \\(\\lambda\\) is nonnegative.\nIf \\(\\lambda \\rightarrow \\infty\\), the regression line becomes a straight line.\nSmoothing splines is a shrunken version of a natural cubic spline with the region outside of the extreme knots perfectly linear.\n\nChoosing the Smoothing Parameter \\(\\lambda\\)\nDegrees of freedom: the number of free parameters\nEffective degrees of freedom \\(df_\\lambda\\): a measure of the flexibility of the smoothing spline.\n\\[\n\\hat{g}_\\lambda = S_\\lambda y\n\\]\n\\(\\hat{g}_\\lambda\\) indicates the fitted value of the smoothing spline.\n\\(S_\\lambda\\) is an n x n matrix.\nThen, the effective degrees of freedom is defined as\n\\[\ndf_\\lambda = \\sum_{i=1}^{n} \\{S_\\lambda\\}_{ii}\n\\]\nTo choose \\(\\lambda\\), cross-validation works. Especially, LOOCV can be computed very efficiently for smoothing splines.\n\n\\(\\hat{g}_\\lambda ^{(-i)}(x_i)\\) indicates the fitted value for the smoothing spline evaluated at \\(x_i\\), where the fit uses all the training data except for the ith observation."
  },
  {
    "objectID": "7.beyond-linearity.html#local-regression",
    "href": "7.beyond-linearity.html#local-regression",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Local Regression",
    "text": "Local Regression\nFit a target point \\(x_0\\) using only the nearby training observations.\n\nGather the fraction \\(s=k/n\\) of training points whose \\(x_i\\) are closest to \\(x_0\\).\nAssign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point, so that the point furthest from \\(x_0\\) has weight zero and the closest has the highest weight.\nFit a weighted least squares regression:\n\n\n\nThe fitted value at \\(x_0\\) is give by \\(\\hat{f}(x0) = \\hat{\\beta_0} + \\hat{\\beta_1} x_0\\).\n\nThe span s plays a significant role to determine smoothness. The larger s will lead to a global filt to the data using all of the training data.\nTo choose s, cross-validation works."
  },
  {
    "objectID": "7.beyond-linearity.html#generalized-additive-models-gam",
    "href": "7.beyond-linearity.html#generalized-additive-models-gam",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Generalized Additive Models (GAM)",
    "text": "Generalized Additive Models (GAM)\nQuantitive Regression\n\nQualitative Classification"
  },
  {
    "objectID": "12.unsupervised.html",
    "href": "12.unsupervised.html",
    "title": "Chapter 12: Unsupervised Learning",
    "section": "",
    "text": "principal components analysis\nclustering"
  },
  {
    "objectID": "12.unsupervised.html#principal-components-analysis",
    "href": "12.unsupervised.html#principal-components-analysis",
    "title": "Chapter 12: Unsupervised Learning",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\nPCA finds a low-dimensional representation of a data set that contains as much as possible of the variation.\nThe first principal component is the normalized linear combination of the features with the largest variance:\n\n\\(\\phi\\) is the loading for each feature. \\(\\sum_{j=1}^{p} \\phi_{j1} = 1\\) by normalized.\nHere is the procedure to calculate the first principal component.\n\nStandardize X. (Make the mean of X equal 0 with standard deviation one.)\n\nSometimes, we just center X without standardization for data with all the features in the same units.\n\n\n\n\\(\\sum_{j=1}^{p} \\phi_{j1} x_{ij} = Z_1\\)\nThis problem can be solved via an eigen decomposition.\n\n\nThen, the second principal component is the linear combination of each feature with maximal variance out of all linear combinations that are uncorrelated with \\(Z_1\\), i.e., orthogonal to the direction \\(\\phi_1\\).\n\n\nThe Proportion of Variance Explained (PVE)\nThe PVE of the mth principal component is defined as\n\nThe variance o the data can be decomposed into the variance of the first M principal components plus the mean squared error of this M-dimensional approximation:\n\nThe principal components can be equivalently viewed as minimizing the approximation error or maximizing the variance.\nThe PVE also can be defined as :\n\n\n\nWe can interpret the PVE as the \\(R^2\\).\n\n\n\n\nHow to determine the number of principal components\nNo simple answer. We can refer to a scree plot below and determine the number subjectively.\n\nOnly if we use PCA for regression, we can tune the number as parameter by cross-validation."
  },
  {
    "objectID": "12.unsupervised.html#missing-values-and-matrix-completion",
    "href": "12.unsupervised.html#missing-values-and-matrix-completion",
    "title": "Chapter 12: Unsupervised Learning",
    "section": "Missing Values and Matrix Completion",
    "text": "Missing Values and Matrix Completion\nImpute the missing values through matrix completion.\n\nCreate a complete data matrix as follows.\n\n\nwhere \\(O\\) is the set of all observed pairs of indices (i,j).\n\nRepeat the following two steps until the objective  fails to decrease.\n\nSolve\n\n\nFor each element \\((i,j) \\notin O\\), set  .\n\n\nReturn the estimated missing entries.\n\nWe can use this technique for recommender system where lot of missing values are in the customer and item rating matrix. For example, we can interpret the M components like this:\n\n\\(\\hat{a}_{im}\\) represents the strength with which the ith user belongs to the mth clique.\n\\(\\hat{b}_{jm}\\) represents the strength with which the jth movie belongs to the mth genre."
  },
  {
    "objectID": "12.unsupervised.html#clustering-methods",
    "href": "12.unsupervised.html#clustering-methods",
    "title": "Chapter 12: Unsupervised Learning",
    "section": "Clustering Methods",
    "text": "Clustering Methods\n\nK-means clustering: partition the observations into a pre-specified number of clusters.\nhierarchical clustering: not know in advance how many clusters we want. (Ended up with A tree-like visual representation called dendrogram.)\n\n\nK-Means Clustering\n\nStandardize feature variables.\nSpecify the desired number of clusters K.\nAssign each observation to one of the K clusters which minimizes the following value.\n\n where \\(W(C_k)\\) represents the within-cluster variance.\nThe most common way to calculate the within-cluster variance involves squared Euclidean distance:\nRandomly assign a number from 1 to K to each of the observations.\nIterate until the cluster assignments stop changing to find a local optimum. (It is difficult to solve a global optimum though.)\n\nCompute the cluster centroid for each cluster. (This is why it is called K-“Mean”s.)\nAssign each observation to the cluster whose centroid is closest.\n\n\nReiterate procedure 2 to find a better local optimum, then pick the best one.\n\nThe disadvantage of K-means is that we need to specify K.\n\n\nHierarchical Clustering\nIt does not require to specify K in advance.\n\nStandardize feature variables.\nCalculate all the pairwise dissimilarities.\n\nEuclidean distance\nCorrelation-based distance (e.g., preferable for recommender system on E-commerce, where we want to cluster users with a similar preference regardless of the amount of past purchases.)\n\nFor i = n, n-1, …,2:\n\nFuse the pair of clusters with the least dissimilarity.\nCompute the new pairwise inter-cluster dissimilarities among the i-1 remaining clusters.\n\n\n\nThe vertical height indicates how close clusters are. We can determine the cutting height to generate arbitrary number of clusters.\nThere are four types of linkage to measure dissimilarity.\n\nComplete: maximal inter-cluster dissimilarity.\nSingle: minimal inter-cluster dissimilarity.\nAverage: mean inter-cluster dissimilarity.\nCentroid: dissimilarity between the centroid for cluster A and B.\n\nGenerally, we prefer Average and Complete to Single. Centroid is often used in genomics, but suffers from a major drawback of an inversion.\n\n\nPractical Issues in Clustering\n\nSmall Decisions with Big Consequences\n\neach choice of parameters has a significant impact on the result, e.g., standardize?, dissimilarity measure, linkage, the number of K.\nSo try various parameters and see the results.\n\nValidating the Clusters Obtained\n\ndifficult to validate the clustering results.\n\nClusters can be heavily distorted by outliers."
  },
  {
    "objectID": "12.unsupervised.html#lab",
    "href": "12.unsupervised.html#lab",
    "title": "Chapter 12: Unsupervised Learning",
    "section": "Lab",
    "text": "Lab\n\nPrincipal Component Analysis\n\nstates <- row.names(USArrests)\nstates\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n\n\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n\n\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n\nUse prcomp to perform PCA. scale = TRUE enables standardization.\n\npr.out <- prcomp(USArrests , scale = TRUE)\n\n\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out$center\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\n\npr.out$scale\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\nrotation indicates the loading vectors.\n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n\n\n\ndim(USArrests)\n\n[1] 50  4\n\ndim(pr.out$x)\n\n[1] 50  4\n\n\n\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\nhead(pr.out$x)\n\n                  PC1        PC2         PC3          PC4\nAlabama    -0.9756604  1.1220012 -0.43980366  0.154696581\nAlaska     -1.9305379  1.0624269  2.01950027 -0.434175454\nArizona    -1.7454429 -0.7384595  0.05423025 -0.826264240\nArkansas    0.1399989  1.1085423  0.11342217 -0.180973554\nCalifornia -2.4986128 -1.5274267  0.59254100 -0.338559240\nColorado   -1.4993407 -0.9776297  1.08400162  0.001450164\n\n\n\nbiplot(pr.out, scale = 0)\n\n\n\n\nThe following image is a mirror of the one above.\n\npr.out$rotation = -pr.out$rotation\npr.out$x = -pr.out$x\nbiplot(pr.out , scale = 0)\n\n\n\n\n\npr.out$sdev # sd of each principal component\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n\n\npr.var <- pr.out$sdev^2\npr.var\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n\n\npve <- pr.var / sum(pr.var)\npve\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\n\n\npar(mfrow = c(1, 2))\nplot(pve , xlab = \"Principal Component\", ylab = \"Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\nplot(cumsum(pve), xlab = \"Principal Component\", ylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\nMatrix Completion\n\nX <- data.matrix(scale(USArrests)) # standardization\npcob <- prcomp(X)\nsummary(pcob)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.5749 0.9949 0.59713 0.41645\nProportion of Variance 0.6201 0.2474 0.08914 0.04336\nCumulative Proportion  0.6201 0.8675 0.95664 1.00000\n\n\n\nsX = svd(X) # the singular value decomposition (SVD)\nnames(sX)\n\n[1] \"d\" \"u\" \"v\"\n\nround(sX$v, 3) # The matrix v is equivalent to the loading matrix from principal components.\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] -0.536  0.418 -0.341  0.649\n[2,] -0.583  0.188 -0.268 -0.743\n[3,] -0.278 -0.873 -0.378  0.134\n[4,] -0.543 -0.167  0.818  0.089\n\n\n\npcob$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n\n\n\nt(sX$d * t(sX$u)) # the matrix u is the matrix o standardized scores. the vecor d is the standard deviaitons.\n\n             [,1]        [,2]        [,3]         [,4]\n [1,] -0.97566045  1.12200121 -0.43980366  0.154696581\n [2,] -1.93053788  1.06242692  2.01950027 -0.434175454\n [3,] -1.74544285 -0.73845954  0.05423025 -0.826264240\n [4,]  0.13999894  1.10854226  0.11342217 -0.180973554\n [5,] -2.49861285 -1.52742672  0.59254100 -0.338559240\n [6,] -1.49934074 -0.97762966  1.08400162  0.001450164\n [7,]  1.34499236 -1.07798362 -0.63679250 -0.117278736\n [8,] -0.04722981 -0.32208890 -0.71141032 -0.873113315\n [9,] -2.98275967  0.03883425 -0.57103206 -0.095317042\n[10,] -1.62280742  1.26608838 -0.33901818  1.065974459\n[11,]  0.90348448 -1.55467609  0.05027151  0.893733198\n[12,]  1.62331903  0.20885253  0.25719021 -0.494087852\n[13,] -1.36505197 -0.67498834 -0.67068647 -0.120794916\n[14,]  0.50038122 -0.15003926  0.22576277  0.420397595\n[15,]  2.23099579 -0.10300828  0.16291036  0.017379470\n[16,]  0.78887206 -0.26744941  0.02529648  0.204421034\n[17,]  0.74331256  0.94880748 -0.02808429  0.663817237\n[18,] -1.54909076  0.86230011 -0.77560598  0.450157791\n[19,]  2.37274014  0.37260865 -0.06502225 -0.327138529\n[20,] -1.74564663  0.42335704 -0.15566968 -0.553450589\n[21,]  0.48128007 -1.45967706 -0.60337172 -0.177793902\n[22,] -2.08725025 -0.15383500  0.38100046  0.101343128\n[23,]  1.67566951 -0.62590670  0.15153200  0.066640316\n[24,] -0.98647919  2.36973712 -0.73336290  0.213342049\n[25,] -0.68978426 -0.26070794  0.37365033  0.223554811\n[26,]  1.17353751  0.53147851  0.24440796  0.122498555\n[27,]  1.25291625 -0.19200440  0.17380930  0.015733156\n[28,] -2.84550542 -0.76780502  1.15168793  0.311354436\n[29,]  2.35995585 -0.01790055  0.03648498 -0.032804291\n[30,] -0.17974128 -1.43493745 -0.75677041  0.240936580\n[31,] -1.96012351  0.14141308  0.18184598 -0.336121113\n[32,] -1.66566662 -0.81491072 -0.63661186 -0.013348844\n[33,] -1.11208808  2.20561081 -0.85489245 -0.944789648\n[34,]  2.96215223  0.59309738  0.29824930 -0.251434626\n[35,]  0.22369436 -0.73477837 -0.03082616  0.469152817\n[36,]  0.30864928 -0.28496113 -0.01515592  0.010228476\n[37,] -0.05852787 -0.53596999  0.93038718 -0.235390872\n[38,]  0.87948680 -0.56536050 -0.39660218  0.355452378\n[39,]  0.85509072 -1.47698328 -1.35617705 -0.607402746\n[40,] -1.30744986  1.91397297 -0.29751723 -0.130145378\n[41,]  1.96779669  0.81506822  0.38538073 -0.108470512\n[42,] -0.98969377  0.85160534  0.18619262  0.646302674\n[43,] -1.34151838 -0.40833518 -0.48712332  0.636731051\n[44,]  0.54503180 -1.45671524  0.29077592 -0.081486749\n[45,]  2.77325613  1.38819435  0.83280797 -0.143433697\n[46,]  0.09536670  0.19772785  0.01159482  0.209246429\n[47,]  0.21472339 -0.96037394  0.61859067 -0.218628161\n[48,]  2.08739306  1.41052627  0.10372163  0.130583080\n[49,]  2.05881199 -0.60512507 -0.13746933  0.182253407\n[50,]  0.62310061  0.31778662 -0.23824049 -0.164976866\n\n\n\npcob$x\n\n                       PC1         PC2         PC3          PC4\nAlabama        -0.97566045  1.12200121 -0.43980366  0.154696581\nAlaska         -1.93053788  1.06242692  2.01950027 -0.434175454\nArizona        -1.74544285 -0.73845954  0.05423025 -0.826264240\nArkansas        0.13999894  1.10854226  0.11342217 -0.180973554\nCalifornia     -2.49861285 -1.52742672  0.59254100 -0.338559240\nColorado       -1.49934074 -0.97762966  1.08400162  0.001450164\nConnecticut     1.34499236 -1.07798362 -0.63679250 -0.117278736\nDelaware       -0.04722981 -0.32208890 -0.71141032 -0.873113315\nFlorida        -2.98275967  0.03883425 -0.57103206 -0.095317042\nGeorgia        -1.62280742  1.26608838 -0.33901818  1.065974459\nHawaii          0.90348448 -1.55467609  0.05027151  0.893733198\nIdaho           1.62331903  0.20885253  0.25719021 -0.494087852\nIllinois       -1.36505197 -0.67498834 -0.67068647 -0.120794916\nIndiana         0.50038122 -0.15003926  0.22576277  0.420397595\nIowa            2.23099579 -0.10300828  0.16291036  0.017379470\nKansas          0.78887206 -0.26744941  0.02529648  0.204421034\nKentucky        0.74331256  0.94880748 -0.02808429  0.663817237\nLouisiana      -1.54909076  0.86230011 -0.77560598  0.450157791\nMaine           2.37274014  0.37260865 -0.06502225 -0.327138529\nMaryland       -1.74564663  0.42335704 -0.15566968 -0.553450589\nMassachusetts   0.48128007 -1.45967706 -0.60337172 -0.177793902\nMichigan       -2.08725025 -0.15383500  0.38100046  0.101343128\nMinnesota       1.67566951 -0.62590670  0.15153200  0.066640316\nMississippi    -0.98647919  2.36973712 -0.73336290  0.213342049\nMissouri       -0.68978426 -0.26070794  0.37365033  0.223554811\nMontana         1.17353751  0.53147851  0.24440796  0.122498555\nNebraska        1.25291625 -0.19200440  0.17380930  0.015733156\nNevada         -2.84550542 -0.76780502  1.15168793  0.311354436\nNew Hampshire   2.35995585 -0.01790055  0.03648498 -0.032804291\nNew Jersey     -0.17974128 -1.43493745 -0.75677041  0.240936580\nNew Mexico     -1.96012351  0.14141308  0.18184598 -0.336121113\nNew York       -1.66566662 -0.81491072 -0.63661186 -0.013348844\nNorth Carolina -1.11208808  2.20561081 -0.85489245 -0.944789648\nNorth Dakota    2.96215223  0.59309738  0.29824930 -0.251434626\nOhio            0.22369436 -0.73477837 -0.03082616  0.469152817\nOklahoma        0.30864928 -0.28496113 -0.01515592  0.010228476\nOregon         -0.05852787 -0.53596999  0.93038718 -0.235390872\nPennsylvania    0.87948680 -0.56536050 -0.39660218  0.355452378\nRhode Island    0.85509072 -1.47698328 -1.35617705 -0.607402746\nSouth Carolina -1.30744986  1.91397297 -0.29751723 -0.130145378\nSouth Dakota    1.96779669  0.81506822  0.38538073 -0.108470512\nTennessee      -0.98969377  0.85160534  0.18619262  0.646302674\nTexas          -1.34151838 -0.40833518 -0.48712332  0.636731051\nUtah            0.54503180 -1.45671524  0.29077592 -0.081486749\nVermont         2.77325613  1.38819435  0.83280797 -0.143433697\nVirginia        0.09536670  0.19772785  0.01159482  0.209246429\nWashington      0.21472339 -0.96037394  0.61859067 -0.218628161\nWest Virginia   2.08739306  1.41052627  0.10372163  0.130583080\nWisconsin       2.05881199 -0.60512507 -0.13746933  0.182253407\nWyoming         0.62310061  0.31778662 -0.23824049 -0.164976866\n\n\n\nnomit <- 20\nset.seed (15)\nina <- sample(seq (50) , nomit)\ninb <- sample (1:4, nomit , replace = TRUE)\nXna <- X\nindex.na <- cbind(ina , inb)\nXna[index.na] <- NA\n\n\nfit.svd <- function(X, M = 1) { \n  svdob <-svd(X)\n  with(svdob ,\n    u[, 1:M, drop = FALSE] %*%\n    (d[1:M] * t(v[, 1:M, drop = FALSE ]))\n  )\n}\n\n\nXhat <- Xna\nxbar <- colMeans(Xna , na.rm = TRUE)\nXhat[index.na] <- xbar[inb]\n\n\nthresh <- 1e-7\nrel_err <- 1\niter <- 0\nismiss <- is.na(Xna)\nmssold <- mean (( scale(Xna , xbar , FALSE)[!ismiss])^2)\nmss0 <- mean(Xna[!ismiss ]^2)\n\n\nwhile(rel_err > thresh) {\n  iter <- iter + 1\n  # Step 2(a)\n  Xapp <- fit.svd(Xhat , M = 1)\n  # Step 2(b)\n  Xhat [ ismiss ] <- Xapp [ ismiss ]\n  # Step 2(c)\n  mss <- mean ((( Xna - Xapp)[! ismiss ])^2)\n  rel_err <- ( mssold - mss ) / mss0\n  mssold <- mss\n  cat(\"Iter:\", iter, \"MSS:\", mss,\n  \"Rel. Err:\", rel_err, \"\\n\")\n}\n\nIter: 1 MSS: 0.3821695 Rel. Err: 0.6194004 \nIter: 2 MSS: 0.3705046 Rel. Err: 0.01161265 \nIter: 3 MSS: 0.3692779 Rel. Err: 0.001221144 \nIter: 4 MSS: 0.3691229 Rel. Err: 0.0001543015 \nIter: 5 MSS: 0.3691008 Rel. Err: 2.199233e-05 \nIter: 6 MSS: 0.3690974 Rel. Err: 3.376005e-06 \nIter: 7 MSS: 0.3690969 Rel. Err: 5.465067e-07 \nIter: 8 MSS: 0.3690968 Rel. Err: 9.253082e-08 \n\n\n\ncor(Xapp[ismiss], X[ismiss])\n\n[1] 0.6535043\n\n\n\n\nK-Means Clustering\n\nset.seed (2)\nx <- matrix(rnorm (50 * 2), ncol = 2)\nx[1:25, 1] <- x[1:25, 1] + 3\nx[1:25, 2] <- x[1:25, 2] - 4\n\n\nkm.out <- kmeans(x, 2, nstart = 20) # nstart: how many random sets should be chosen (the number of iteratins)\n\n\nkm.out$cluster\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\n\npar(mfrow = c(1, 2))\nplot(x, col = (km.out$cluster + 1), \n     main = \"K-Means Clustering Results with K = 2\", \n     xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n\n\n\n\n\nset.seed (4)\nkm.out <- kmeans(x, 3, nstart = 20)\nkm.out\n\nK-means clustering with 3 clusters of sizes 17, 23, 10\n\nCluster means:\n        [,1]        [,2]\n1  3.7789567 -4.56200798\n2 -0.3820397 -0.08740753\n3  2.3001545 -2.69622023\n\nClustering vector:\n [1] 1 3 1 3 1 1 1 3 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 3 2 3 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 25.74089 52.67700 19.56137\n (between_SS / total_SS =  79.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nplot(x, col = (km.out$cluster + 1),\n     main = \"K-Means Clustering Results with K = 3\",\n     xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n\n\n\n\n\nset.seed (4)\nkm.out <- kmeans(x, 3, nstart = 1)\nkm.out$tot.withinss # the total within-cluster sum of squares\n\n[1] 104.3319\n\nkm.out <- kmeans(x, 3, nstart = 20)\nkm.out$tot.withinss\n\n[1] 97.97927\n\n\n\n\nHierarchical Clustering\n\nhc.complete <- hclust(dist(x), method = \"complete\")\n\ndist computes the 50 x 50 inter-observation Euclidean distance matrix.\n\nhc.average <- hclust(dist(x), method = \"average\")\nhc.single <- hclust(dist(x), method = \"single\")\n\n\npar(mfrow = c(1, 3))\nplot(hc.complete , main = \"Complete Linkage\",\n     xlab = \"\", sub = \"\", cex = .9)\nplot(hc.average , main = \"Average Linkage\",\n     xlab = \"\", sub = \"\", cex = .9)\nplot(hc.single, main = \"Single Linkage\",\n     xlab = \"\", sub = \"\", cex = .9)\n\n\n\n\n\ncutree(hc.complete , 2)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\ncutree(hc.average , 2)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2\n[39] 2 2 2 2 2 1 2 1 2 2 2 2\n\ncutree(hc.single, 2)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\nxsc <- scale(x)\nplot(hclust(dist(xsc), method = \"complete\"),\n     main = \"Hierarchical Clustering with Scaled Features\")\n\n\n\n\n\nx <- matrix(rnorm (30 * 3), ncol = 3)\ndd <- as.dist (1 - cor(t(x)))\nplot(hclust(dd, method = \"complete\"),\n     main = \"Complete Linkage with Correlation -Based Distance\",\n     xlab = \"\", sub = \"\")"
  },
  {
    "objectID": "12.unsupervised.html#nci60-data-example",
    "href": "12.unsupervised.html#nci60-data-example",
    "title": "Chapter 12: Unsupervised Learning",
    "section": "NCI60 Data example",
    "text": "NCI60 Data example\n\nlibrary(ISLR2)\nnci.labs <- NCI60$labs\nnci.data <- NCI60$data\n\n\npr.out <- prcomp(nci.data , scale = TRUE)\n\n\nCols <- function(vec) {\n  cols <-rainbow(length(unique(vec)))\n  return(cols[as.numeric(as.factor(vec))])\n}\n\n\npar(mfrow = c(1, 2))\nplot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,\n     xlab = \"Z1\", ylab = \"Z2\")\nplot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,\n     xlab = \"Z1\", ylab = \"Z3\")\n\n\n\n\n\nsummary(pr.out)\n\nImportance of components:\n                           PC1      PC2      PC3      PC4      PC5      PC6\nStandard deviation     27.8535 21.48136 19.82046 17.03256 15.97181 15.72108\nProportion of Variance  0.1136  0.06756  0.05752  0.04248  0.03735  0.03619\nCumulative Proportion   0.1136  0.18115  0.23867  0.28115  0.31850  0.35468\n                            PC7      PC8      PC9     PC10     PC11     PC12\nStandard deviation     14.47145 13.54427 13.14400 12.73860 12.68672 12.15769\nProportion of Variance  0.03066  0.02686  0.02529  0.02376  0.02357  0.02164\nCumulative Proportion   0.38534  0.41220  0.43750  0.46126  0.48482  0.50646\n                           PC13     PC14     PC15     PC16     PC17     PC18\nStandard deviation     11.83019 11.62554 11.43779 11.00051 10.65666 10.48880\nProportion of Variance  0.02049  0.01979  0.01915  0.01772  0.01663  0.01611\nCumulative Proportion   0.52695  0.54674  0.56590  0.58361  0.60024  0.61635\n                           PC19    PC20     PC21    PC22    PC23    PC24\nStandard deviation     10.43518 10.3219 10.14608 10.0544 9.90265 9.64766\nProportion of Variance  0.01594  0.0156  0.01507  0.0148 0.01436 0.01363\nCumulative Proportion   0.63229  0.6479  0.66296  0.6778 0.69212 0.70575\n                          PC25    PC26    PC27   PC28    PC29    PC30    PC31\nStandard deviation     9.50764 9.33253 9.27320 9.0900 8.98117 8.75003 8.59962\nProportion of Variance 0.01324 0.01275 0.01259 0.0121 0.01181 0.01121 0.01083\nCumulative Proportion  0.71899 0.73174 0.74433 0.7564 0.76824 0.77945 0.79027\n                          PC32    PC33    PC34    PC35    PC36    PC37    PC38\nStandard deviation     8.44738 8.37305 8.21579 8.15731 7.97465 7.90446 7.82127\nProportion of Variance 0.01045 0.01026 0.00988 0.00974 0.00931 0.00915 0.00896\nCumulative Proportion  0.80072 0.81099 0.82087 0.83061 0.83992 0.84907 0.85803\n                          PC39    PC40    PC41   PC42    PC43   PC44    PC45\nStandard deviation     7.72156 7.58603 7.45619 7.3444 7.10449 7.0131 6.95839\nProportion of Variance 0.00873 0.00843 0.00814 0.0079 0.00739 0.0072 0.00709\nCumulative Proportion  0.86676 0.87518 0.88332 0.8912 0.89861 0.9058 0.91290\n                         PC46    PC47    PC48    PC49    PC50    PC51    PC52\nStandard deviation     6.8663 6.80744 6.64763 6.61607 6.40793 6.21984 6.20326\nProportion of Variance 0.0069 0.00678 0.00647 0.00641 0.00601 0.00566 0.00563\nCumulative Proportion  0.9198 0.92659 0.93306 0.93947 0.94548 0.95114 0.95678\n                          PC53    PC54    PC55    PC56    PC57   PC58    PC59\nStandard deviation     6.06706 5.91805 5.91233 5.73539 5.47261 5.2921 5.02117\nProportion of Variance 0.00539 0.00513 0.00512 0.00482 0.00438 0.0041 0.00369\nCumulative Proportion  0.96216 0.96729 0.97241 0.97723 0.98161 0.9857 0.98940\n                          PC60    PC61    PC62    PC63      PC64\nStandard deviation     4.68398 4.17567 4.08212 4.04124 2.148e-14\nProportion of Variance 0.00321 0.00255 0.00244 0.00239 0.000e+00\nCumulative Proportion  0.99262 0.99517 0.99761 1.00000 1.000e+00\n\n\n\nplot(pr.out)\n\n\n\n\n\npve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev ^2)\npar(mfrow = c(1, 2))\nplot(pve , type = \"o\", ylab = \"PVE\",\n     xlab = \"Principal Component\", col = \"blue\")\nplot(cumsum(pve), type = \"o\", ylab = \"Cumulative PVE\",\n     xlab = \"Principal Component\", col = \"brown3\")\n\n\n\n\n\nsd.data <- scale(nci.data)\n\n\npar(mfrow = c(1, 3))\ndata.dist <- dist(sd.data)\nplot(hclust(data.dist), xlab = \"\", sub = \"\", ylab = \"\",\n     labels = nci.labs , main = \"Complete Linkage\")\nplot(hclust(data.dist , method = \"average\"),\n     labels = nci.labs , main = \"Average Linkage\",\n     xlab = \"\", sub = \"\", ylab = \"\")\nplot(hclust(data.dist , method = \"single\"),\n     labels = nci.labs , main = \"Single Linkage\",\n     xlab = \"\", sub = \"\", ylab = \"\")\n\n\n\n\n\nhc.out <- hclust(dist(sd.data))\nhc.clusters <- cutree(hc.out , 4)\ntable(hc.clusters , nci.labs)\n\n           nci.labs\nhc.clusters BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro\n          1      2   3     2           0           0        0           0\n          2      3   2     0           0           0        0           0\n          3      0   0     0           1           1        6           0\n          4      2   0     5           0           0        0           1\n           nci.labs\nhc.clusters MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN\n          1           0        8     8       6        2     8       1\n          2           0        0     1       0        0     1       0\n          3           0        0     0       0        0     0       0\n          4           1        0     0       0        0     0       0\n\n\n\npar(mfrow = c(1, 1))\nplot(hc.out , labels = nci.labs)\nabline(h = 139, col = \"red\")\n\n\n\n\n\nhc.out\n\n\nCall:\nhclust(d = dist(sd.data))\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 64 \n\n\n\nset.seed (2)\nkm.out <- kmeans(sd.data, 4, nstart = 20)\nkm.clusters <- km.out$cluster\ntable(km.clusters , hc.clusters)\n\n           hc.clusters\nkm.clusters  1  2  3  4\n          1 11  0  0  9\n          2 20  7  0  0\n          3  9  0  0  0\n          4  0  0  8  0\n\nhc.clusters\n\n V1  V2  V3  V4  V5  V6  V7  V8  V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 \n  1   1   1   1   2   2   2   2   1   1   1   1   1   1   1   1   1   2   2   2 \nV21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   3   3   3   3   3   3   3 \nV41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 \n  3   1   4   1   4   4   4   4   4   4   4   4   1   1   1   1   1   1   1   1 \nV61 V62 V63 V64 \n  1   1   1   1 \n\n\n\nhc.out <- hclust(dist(pr.out$x[, 1:5]))\nplot(hc.out , labels = nci.labs ,\n     main = \"Hier. Clust. on First Five Score Vectors\")\n\n\n\ntable(cutree(hc.out , 4), nci.labs)\n\n   nci.labs\n    BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro MCF7D-repro\n  1      0   2     7           0           0        2           0           0\n  2      5   3     0           0           0        0           0           0\n  3      0   0     0           1           1        4           0           0\n  4      2   0     0           0           0        0           1           1\n   nci.labs\n    MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN\n  1        1     8       5        2     7       0\n  2        7     1       1        0     2       1\n  3        0     0       0        0     0       0\n  4        0     0       0        0     0       0"
  }
]