[
  {
    "objectID": "5.resampling.html",
    "href": "5.resampling.html",
    "title": "Chapter 5: Resampling Methods",
    "section": "",
    "text": "cross-validation\nbootstrap"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "This is a note for the book “An Introduction to Statistical Learning.”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "5.resampling.html#cross-validation",
    "href": "5.resampling.html#cross-validation",
    "title": "Chapter 5: Resampling Methods",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nThe Validation Set Approach\nSplit data into the following two sets\n\na training set\na validation set (hold-out set)\n\nWe can estimate the test error rate from the validation set error rate, which can be highly variable.\n\n\nLeave-One-Out Cross-Validation (LOOCV)\nLOOCV addresses the high variability of the validation set approach.\nWith least squares linear or polynomial regression, the cost of LOOCV is the same as that of a single model.\n\n\n\nk-Fold Cross-Validation\nIt randomly divides the set of observations into k groups.\n\nLOOCV is the special case of k-fold CV in which k is set to equal n.\n\nk-fold CV with k < n has a computational advantage to LOOCV.\nLOOCV reduces more bias than k-fold CV.\nLOOCV has higher variance than k-fold CV.\n\neach of n models is trained on an almost identical set of observations.\n\nk-fold CV often gives more accurate estimates of the test error rate than LOOCV.\n\nSo typically, k = 5 or k = 10 tends to yield more accurate test error rate estimates.\n\n\nCross-Validation on Classification Problems\n\n\\[Err_i = I(y_i \\neq \\hat{y_i})\\]"
  },
  {
    "objectID": "5.resampling.html#the-bootstrap",
    "href": "5.resampling.html#the-bootstrap",
    "title": "Chapter 5: Resampling Methods",
    "section": "The Bootstrap",
    "text": "The Bootstrap\nit is used to quantify the uncertainty associated with a given estimator.\nE.g., the bootstrap can be used to estimate the standard errors of the coefficients."
  },
  {
    "objectID": "5.resampling.html#lab",
    "href": "5.resampling.html#lab",
    "title": "Chapter 5: Resampling Methods",
    "section": "Lab",
    "text": "Lab\n\nThe Validation Set Approach\n\nlibrary(ISLR2)\nset.seed(1)\ntrain = sample(392, 196)\n\nSimple Linear Regression Model:\n\nlm.fit = lm(mpg ~ horsepower, data = Auto, subset = train)\n\n\nattach(Auto)\nmean((mpg - predict(lm.fit, Auto))[-train]^2) # the estimated test MSE\n\n[1] 23.26601\n\n\npolynomial regression models:\n\nlm.fit2 = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2) # the estimated test MSE\n\n[1] 18.71646\n\n\n\nlm.fit3 = lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2) # the estimated test MSE\n\n[1] 18.79401\n\n\nWe prefer the quadratic regression model based on the validation MSE.\n\n\nLOOCV\n\nglm.fit = glm(mpg ~ horsepower, data = Auto)\ncoef(glm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\n\nlibrary(boot)\ncv.err = cv.glm(Auto, glm.fit)\ncv.err$delta # [1]: the standard k-fold CV estimate, [2]: a bias-corrected version\n\n[1] 24.23151 24.23114\n\n\n\ncv.error = rep(0, 10)\nfor(i in 1:10){\n  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093\n\n\n\nplot(cv.error)\n\n\n\n\nWe see a sharp drop in the estimated test MSE between the linear and quadratic fits, while not from higher-order polynomials.\n\n\nk-Fold Cross Validation\n\nset.seed(17)\ncv.error.10 = rep(0, 10)\nfor (i in 1:10) {\n  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\nplot(cv.error.10)\n\n\n\n\nHigher-order polynomials than quadratic does not show improvement of test MSE.\n\n\nThe Bootstrap\n\nhead(Portfolio)\n\n           X          Y\n1 -0.8952509 -0.2349235\n2 -1.5624543 -0.8851760\n3 -0.4170899  0.2718880\n4  1.0443557 -0.7341975\n5 -0.3155684  0.8419834\n6 -1.7371238 -2.0371910\n\n\n\nalpha.fn = function(data, index){\n  X = data$X[index]\n  Y = data$Y[index]\n  (var(Y) - cov(X, Y) / (var(X) + var(Y) - 2 * cov(X, Y)))\n}\n\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.7792916\n\n\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\n[1] 0.9142495\n\n\n\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.7792916 -0.02651568   0.1723508\n\n\n\nboot.fn = function(data, index){\n  coef(lm(mpg ~ horsepower, data= data, subset = index))\n}\n\n\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\n\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0549915227 0.841925746\nt2* -0.1578447 -0.0006210818 0.007348956\n\n\n\nsummary(lm(mpg ~ horsepower , data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\n\nboot.fn2 = function(data, index){\n  coef(lm(mpg ~ horsepower + I(horsepower^2), data= data, subset = index))\n}\n\n\nset.seed(1)\nboot(Auto, boot.fn2, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn2, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\n\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))\n\n\nCall:\nlm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     56.9000997  1.8004268   31.60   <2e-16 ***\nhorsepower      -0.4661896  0.0311246  -14.98   <2e-16 ***\nI(horsepower^2)  0.0012305  0.0001221   10.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "6.model-selection.html",
    "href": "6.model-selection.html",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "",
    "text": "There are three ways to select model."
  },
  {
    "objectID": "6.model-selection.html#subset-selection",
    "href": "6.model-selection.html#subset-selection",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Subset Selection",
    "text": "Subset Selection\nThe exhaustive search entails computational limitations; the number of possible models grows rapidly as p increases.\nSo we take advantage of the following stepwise selection.\n\nForward Stepwise Selection\nBackward Stepwise Selection\nHybrid Approaches of forward and backward\n\nWe determine the best model based on the following statistics.\n\nThe indirect estimates of test error by making an adjustment to the training error\n\n\\(C_p\\)\nBIC\nadjusted \\(R^2\\)\n\nThe direct estimate of the test error\n\ncross-validated prediction error\n\n\nWhile we preferred the indirect estimate in the past with limitation of computational resources, the cross-validation prediction error obtains popularity today.\n\nStepwise Selection\n\\[\nC_p = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2)\n\\]\n\\[\nAIC = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2)\n\\]\n\\[\nBIC = \\frac{1}{n}(RSS + log(n) \\hat{\\sigma}^2)\n\\]\n\\(d \\hat{\\sigma}^2\\) is a penalty for the number of predictors.\nFor the Gaussian model (with variance \\(\\sigma_\\epsilon ^2\\) = \\(\\hat{\\sigma}_\\epsilon ^2\\)), the AIC is equivalent to \\(C_p\\).\n\\[\nAdjusted R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}\n\\]\n\\(d\\) is the number of variables."
  },
  {
    "objectID": "6.model-selection.html#shrinkage-model",
    "href": "6.model-selection.html#shrinkage-model",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Shrinkage Model",
    "text": "Shrinkage Model\n\nRidge Regression: \\(l_2\\) penalty\nLasso: \\(l_1\\) penalty\n\n\nRidge Regression\nInstead of RSS,\n\nthe ridge regression estimates \\(\\hat{\\beta^R}\\) that minimize the following function.\n\n\\(\\lambda \\sum\\beta_j^2\\) is called a shrinkage penalty.\nNote that we do not impose penalty on the intercept.\nIt is best to apply ridge regression after standardizing the predictors since the \\(X_j \\hat{\\beta}^R_{j,\\lambda}\\) is depend on \\(\\lambda\\) and the scaling of the other predictors.\nAs \\(\\lambda\\) increases, the ridge regression leads to a significant reduction in the variance of the predictions at the expense of a slight increase in bias under some value of \\(\\lambda\\). Therefore, the shrinkage tends to generate a better fit for the test dataset than the least squares approach.\nThe disadvantage of the ridge regression is to always include all p predictors.\n\n\nThe Lasso\nThe lasso coefficients minimize the following quantity.\n\nThe \\(l_1\\) penalty forces some of the coefficients to be exactly equal to zero.\nNeither ridge regression nor the lasso will universally dominate the other. The lasso outperform the ridge regression in case that a small number of the predictors are significant. However, we cannot know their significance, so cross-validation can be used to determine which approach is better.\nThe ridge regression shrinks every dimension by the same proportion, whereas the lasso shrinks all coefficients toward zero by a similar amount.\n\n\n\nHow to determine \\(\\lambda\\)\nFind \\(\\lambda\\) from the cross-validation error for each \\(\\lambda\\)."
  },
  {
    "objectID": "6.model-selection.html#dimension-reduction-methods",
    "href": "6.model-selection.html#dimension-reduction-methods",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nPrincipal Components Regression (PCR)\nAn unsupervised method, which is not always the best to predict.\n\nRecommend to standardize all the predictors since the high-variance variables tend to play a larger role .\nConstruct the first M principal components, \\(Z_1, Z_2, …, Z_M\\).\n\nThe first principal component direction is that along which the observations vary the most.\nThe second principal component direction must be perpendicular or orthogonal to the first principal component direction. Under this constraint, it also must have largest variance.\n\nFit the model with the M principal components, which can avoid overfitting than the model with all p variables.\nThe value of M is typically chosen by cross-validation.\n\n\n\nPartial least squares (PLS)\nA supervised method.\n\nStandardize the p predictors.\nConstruct the first M principal components, \\(Z_1, Z_2, …, Z_M\\).\n\nCompute the first direction \\(Z_1\\) by setting each coefficient of X equal to those from the simple linear regression.\nTake residuals between actual values and \\(Z_1\\).\nCompute the second direction \\(Z_2\\) for the residuals with least squares.\n\n\nWhile PLS can reduce bias, it has the potential to increase variance. So it often performs no better than ridge regression or PCR."
  },
  {
    "objectID": "6.model-selection.html#regression-in-high-dimensions",
    "href": "6.model-selection.html#regression-in-high-dimensions",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Regression in High Dimensions",
    "text": "Regression in High Dimensions\nThe model with more predictors than the number of observations fits data exactly, which often leads to overfitting.\nHere are the workarounds.\n\nforward stepwise selection\nridge regression\nthe lasso\nprincipal components regression\n\nAlso, pay attention to the interpretation. If the variables entail multicollinearity, the values of coefficients are uninterpretable."
  },
  {
    "objectID": "6.model-selection.html#lab",
    "href": "6.model-selection.html#lab",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Lab",
    "text": "Lab\n\nlibrary(ISLR2)\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\n\nHitters = na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\n\n\nBest subset selection\n\nlibrary(leaps)\nregfit.full = regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\n\nregfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nreg.summary = summary(regfit.full)\n\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\npar(mfrow = c(2,2))\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\n\nwhich.max(reg.summary$adjr2)\n\n[1] 11\n\n\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\npoints(11, reg.summary$adjr2[11], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\nForward and Backward Stepwise Selection\n\nregfit.fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\n\nregfit.bwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\n\n\nCross-validation\n\nset.seed(1)\ntrain = sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)\ntest = (!train)\n\n\nregfit.best = regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)\n\n\ntest.mat = model.matrix(Salary ~ ., data = Hitters[test, ])\n\n\nval.errors = rep(NA, 19)\nfor(i in 1:19){\n  coefi = coef(regfit.best, id = i)\n  pred = test.mat[, names(coefi)] %*% coefi\n  val.errors[i] = mean((Hitters$Salary[test] - pred)^2)\n}\n\n\ncoef(regfit.best, which.min(val.errors))\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\n\npredict.regsubsets = function(object, newdata, id){\n  form = as.formula(object$call[[2]])\n  mat = model.matrix(form, newdata)\n  coefi = coef(object, id = id)\n  xvars = names(coefi)\n  mat[, xvars] %*% coefi\n}\n\n\nregfit.best = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\n\nk = 10\nn = nrow(Hitters)\nset.seed(1)\nfolds = sample(rep(1:k, length = n))\ncv.errors = matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))\n\n\nfor(j in 1:k){\n  best.fit = regsubsets(Salary ~ ., data = Hitters[folds != j,], nvmax = 19)\n  for (i in 1:19){\n    pred = predict(best.fit, Hitters[folds ==j, ], id = i)\n    cv.errors[j, i] = mean((Hitters$Salary[folds == j] - pred)^2)\n  }\n}\n\n\nmean.cv.errors = apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\n\n\npar(mfrow = c(1,1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\nWe found a 10-variable model is the best through cross-validation.\n\n\nRidge Regression\n\nx = model.matrix(Salary ~ ., Hitters)[, -1]\ny = Hitters$Salary\n\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-4\n\ngrid = 10^seq(10, -2, length = 100)\nridge.mod = glmnet(x, y, alpha = 0, lambda = grid)\n\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\n\n\npredict(ridge.mod, s = 50, type = \"coefficients\") # lambda = 50\n\n20 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  4.876610e+01\nAtBat       -3.580999e-01\nHits         1.969359e+00\nHmRun       -1.278248e+00\nRuns         1.145892e+00\nRBI          8.038292e-01\nWalks        2.716186e+00\nYears       -6.218319e+00\nCAtBat       5.447837e-03\nCHits        1.064895e-01\nCHmRun       6.244860e-01\nCRuns        2.214985e-01\nCRBI         2.186914e-01\nCWalks      -1.500245e-01\nLeagueN      4.592589e+01\nDivisionW   -1.182011e+02\nPutOuts      2.502322e-01\nAssists      1.215665e-01\nErrors      -3.278600e+00\nNewLeagueN  -9.496680e+00\n\n\n\nset.seed(1)\ntrain = sample(1:nrow(x), nrow(x) / 2)\ntest = (-train)\ny.test = y[test]\n\n\nridge.mod = glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)\nridge.pred = predict(ridge.mod, s = 4, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\n\nset.seed(1)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 0)\nplot(cv.out)\n\n\n\n\n\nbestlam = cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\n\nridge.pred = predict(ridge.mod, s = bestlam, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\n\nout = glmnet(x, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\n\n\nThe Lasso\n\nlasso.mod = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\n\nset.seed(1)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\n\n\nbestlam = cv.out$lambda.min\nlasso.pred = predict(lasso.mod, s = bestlam, newx = x[test, ])\nmean((lasso.pred -y.test)^2)\n\n[1] 143673.6\n\n\n\nout = glmnet(x, y, alpha = 1, lambda = grid)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\n\n\n\nPCR regression\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nset.seed(2)\npcr.fit = pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = \"CV\") # ten-fold cross-validation\n\n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    351.9    353.2    355.0    352.8    348.4    343.6\nadjCV          452    351.6    352.7    354.4    352.1    347.6    342.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       345.5    347.7    349.6     351.4     352.1     353.5     358.2\nadjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.7     349.4     339.9     341.6     339.2     339.6\nadjCV     348.0     347.7     338.2     339.7     337.2     337.6\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\n\nvalidationplot(pcr.fit, val.type = \"MSEP\") # cross-validation MSE to be plotted\n\n\n\n\n\nset.seed(1)\npcr.fit = pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\n\npcr.pred = predict(pcr.fit, x[test, ], ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142811.8\n\n\n\npcr.fit = pcr(y ~ x, scale = TRUE, ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90\n\n\n\n\nPartial Least Squares\n\nset.seed(1)\npls.fit = plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nsummary(pls.fit)\n\nData:   X dimension: 131 19 \n    Y dimension: 131 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           428.3    325.5    329.9    328.8    339.0    338.9    340.1\nadjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       339.0    347.1    346.4     343.4     341.5     345.4     356.4\nadjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        348.4     349.1     350.0     344.2     344.5     345.0\nadjCV     344.2     345.0     345.9     340.4     340.6     341.1\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71\nSalary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         93.17     96.05     97.08     97.61     97.97     98.70     99.12\nSalary    55.95     56.12     56.47     56.68     57.37     57.76     58.08\n        16 comps  17 comps  18 comps  19 comps\nX          99.61     99.70     99.95    100.00\nSalary     58.17     58.49     58.56     58.62\n\n\n\nvalidationplot(pls.fit, val.type = \"MSEP\")\n\n\n\n\n\npls.pred = predict(pls.fit, x[test, ], ncomp = 1)\nmean((pls.pred - y.test)^2)\n\n[1] 151995.3\n\n\n\npls.fit = plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 1)\nsummary(pls.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: kernelpls\nNumber of components considered: 1\nTRAINING: % variance explained\n        1 comps\nX         38.08\nSalary    43.05"
  }
]