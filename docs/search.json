[
  {
    "objectID": "5.resampling.html",
    "href": "5.resampling.html",
    "title": "Chapter 5: Resampling Methods",
    "section": "",
    "text": "cross-validation\nbootstrap"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "This is a note for the book “An Introduction to Statistical Learning.”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "5.resampling.html#cross-validation",
    "href": "5.resampling.html#cross-validation",
    "title": "Chapter 5: Resampling Methods",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nThe Validation Set Approach\nSplit data into the following two sets\n\na training set\na validation set (hold-out set)\n\nWe can estimate the test error rate from the validation set error rate, which can be highly variable.\n\n\nLeave-One-Out Cross-Validation (LOOCV)\nLOOCV addresses the high variability of the validation set approach.\nWith least squares linear or polynomial regression, the cost of LOOCV is the same as that of a single model.\n\n\n\nk-Fold Cross-Validation\nIt randomly divides the set of observations into k groups.\n\nLOOCV is the special case of k-fold CV in which k is set to equal n.\n\nk-fold CV with k < n has a computational advantage to LOOCV.\nLOOCV reduces more bias than k-fold CV.\nLOOCV has higher variance than k-fold CV.\n\neach of n models is trained on an almost identical set of observations.\n\nk-fold CV often gives more accurate estimates of the test error rate than LOOCV.\n\nSo typically, k = 5 or k = 10 tends to yield more accurate test error rate estimates.\n\n\nCross-Validation on Classification Problems\n\n\\[Err_i = I(y_i \\neq \\hat{y_i})\\]"
  },
  {
    "objectID": "5.resampling.html#the-bootstrap",
    "href": "5.resampling.html#the-bootstrap",
    "title": "Chapter 5: Resampling Methods",
    "section": "The Bootstrap",
    "text": "The Bootstrap\nit is used to quantify the uncertainty associated with a given estimator.\nE.g., the bootstrap can be used to estimate the standard errors of the coefficients."
  },
  {
    "objectID": "5.resampling.html#lab",
    "href": "5.resampling.html#lab",
    "title": "Chapter 5: Resampling Methods",
    "section": "Lab",
    "text": "Lab\n\nThe Validation Set Approach\n\nlibrary(ISLR2)\nset.seed(1)\ntrain = sample(392, 196)\n\nSimple Linear Regression Model:\n\nlm.fit = lm(mpg ~ horsepower, data = Auto, subset = train)\n\n\nattach(Auto)\nmean((mpg - predict(lm.fit, Auto))[-train]^2) # the estimated test MSE\n\n[1] 23.26601\n\n\npolynomial regression models:\n\nlm.fit2 = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\nmean((mpg - predict(lm.fit2, Auto))[-train]^2) # the estimated test MSE\n\n[1] 18.71646\n\n\n\nlm.fit3 = lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\nmean((mpg - predict(lm.fit3, Auto))[-train]^2) # the estimated test MSE\n\n[1] 18.79401\n\n\nWe prefer the quadratic regression model based on the validation MSE.\n\n\nLOOCV\n\nglm.fit = glm(mpg ~ horsepower, data = Auto)\ncoef(glm.fit)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\n\nlibrary(boot)\ncv.err = cv.glm(Auto, glm.fit)\ncv.err$delta # [1]: the standard k-fold CV estimate, [2]: a bias-corrected version\n\n[1] 24.23151 24.23114\n\n\n\ncv.error = rep(0, 10)\nfor(i in 1:10){\n  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093\n\n\n\nplot(cv.error)\n\n\n\n\nWe see a sharp drop in the estimated test MSE between the linear and quadratic fits, while not from higher-order polynomials.\n\n\nk-Fold Cross Validation\n\nset.seed(17)\ncv.error.10 = rep(0, 10)\nfor (i in 1:10) {\n  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\nplot(cv.error.10)\n\n\n\n\nHigher-order polynomials than quadratic does not show improvement of test MSE.\n\n\nThe Bootstrap\n\nhead(Portfolio)\n\n           X          Y\n1 -0.8952509 -0.2349235\n2 -1.5624543 -0.8851760\n3 -0.4170899  0.2718880\n4  1.0443557 -0.7341975\n5 -0.3155684  0.8419834\n6 -1.7371238 -2.0371910\n\n\n\nalpha.fn = function(data, index){\n  X = data$X[index]\n  Y = data$Y[index]\n  (var(Y) - cov(X, Y) / (var(X) + var(Y) - 2 * cov(X, Y)))\n}\n\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.7792916\n\n\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\n[1] 0.9142495\n\n\n\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.7792916 -0.02651568   0.1723508\n\n\n\nboot.fn = function(data, index){\n  coef(lm(mpg ~ horsepower, data= data, subset = index))\n}\n\n\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\n\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0549915227 0.841925746\nt2* -0.1578447 -0.0006210818 0.007348956\n\n\n\nsummary(lm(mpg ~ horsepower , data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\n\nboot.fn2 = function(data, index){\n  coef(lm(mpg ~ horsepower + I(horsepower^2), data= data, subset = index))\n}\n\n\nset.seed(1)\nboot(Auto, boot.fn2, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn2, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\n\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))\n\n\nCall:\nlm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     56.9000997  1.8004268   31.60   <2e-16 ***\nhorsepower      -0.4661896  0.0311246  -14.98   <2e-16 ***\nI(horsepower^2)  0.0012305  0.0001221   10.08   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "6.model-selection.html",
    "href": "6.model-selection.html",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "",
    "text": "There are three ways to select model."
  },
  {
    "objectID": "6.model-selection.html#subset-selection",
    "href": "6.model-selection.html#subset-selection",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Subset Selection",
    "text": "Subset Selection\nThe exhaustive search entails computational limitations; the number of possible models grows rapidly as p increases.\nSo we take advantage of the following stepwise selection.\n\nForward Stepwise Selection\nBackward Stepwise Selection\nHybrid Approaches of forward and backward\n\nWe determine the best model based on the following statistics.\n\nThe indirect estimates of test error by making an adjustment to the training error\n\n\\(C_p\\)\nBIC\nadjusted \\(R^2\\)\n\nThe direct estimate of the test error\n\ncross-validated prediction error\n\n\nWhile we preferred the indirect estimate in the past with limitation of computational resources, the cross-validation prediction error obtains popularity today.\n\nStepwise Selection\n\\[\nC_p = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2)\n\\]\n\\[\nAIC = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2)\n\\]\n\\[\nBIC = \\frac{1}{n}(RSS + log(n) \\hat{\\sigma}^2)\n\\]\n\\(d \\hat{\\sigma}^2\\) is a penalty for the number of predictors.\nFor the Gaussian model (with variance \\(\\sigma_\\epsilon ^2\\) = \\(\\hat{\\sigma}_\\epsilon ^2\\)), the AIC is equivalent to \\(C_p\\).\n\\[\nAdjusted R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}\n\\]\n\\(d\\) is the number of variables."
  },
  {
    "objectID": "6.model-selection.html#shrinkage-model",
    "href": "6.model-selection.html#shrinkage-model",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Shrinkage Model",
    "text": "Shrinkage Model\n\nRidge Regression: \\(l_2\\) penalty\nLasso: \\(l_1\\) penalty\n\n\nRidge Regression\nInstead of RSS,\n\nthe ridge regression estimates \\(\\hat{\\beta^R}\\) that minimize the following function.\n\n\\(\\lambda \\sum\\beta_j^2\\) is called a shrinkage penalty.\nNote that we do not impose penalty on the intercept.\nIt is best to apply ridge regression after standardizing the predictors since the \\(X_j \\hat{\\beta}^R_{j,\\lambda}\\) is depend on \\(\\lambda\\) and the scaling of the other predictors.\nAs \\(\\lambda\\) increases, the ridge regression leads to a significant reduction in the variance of the predictions at the expense of a slight increase in bias under some value of \\(\\lambda\\). Therefore, the shrinkage tends to generate a better fit for the test dataset than the least squares approach.\nThe disadvantage of the ridge regression is to always include all p predictors.\n\n\nThe Lasso\nThe lasso coefficients minimize the following quantity.\n\nThe \\(l_1\\) penalty forces some of the coefficients to be exactly equal to zero.\nNeither ridge regression nor the lasso will universally dominate the other. The lasso outperform the ridge regression in case that a small number of the predictors are significant. However, we cannot know their significance, so cross-validation can be used to determine which approach is better.\nThe ridge regression shrinks every dimension by the same proportion, whereas the lasso shrinks all coefficients toward zero by a similar amount.\n\n\n\nHow to determine \\(\\lambda\\)\nFind \\(\\lambda\\) from the cross-validation error for each \\(\\lambda\\)."
  },
  {
    "objectID": "6.model-selection.html#dimension-reduction-methods",
    "href": "6.model-selection.html#dimension-reduction-methods",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nPrincipal Components Regression (PCR)\nAn unsupervised method, which is not always the best to predict.\n\nRecommend to standardize all the predictors since the high-variance variables tend to play a larger role .\nConstruct the first M principal components, \\(Z_1, Z_2, …, Z_M\\).\n\nThe first principal component direction is that along which the observations vary the most.\nThe second principal component direction must be perpendicular or orthogonal to the first principal component direction. Under this constraint, it also must have largest variance.\n\nFit the model with the M principal components, which can avoid overfitting than the model with all p variables.\nThe value of M is typically chosen by cross-validation.\n\n\n\nPartial least squares (PLS)\nA supervised method.\n\nStandardize the p predictors.\nConstruct the first M principal components, \\(Z_1, Z_2, …, Z_M\\).\n\nCompute the first direction \\(Z_1\\) by setting each coefficient of X equal to those from the simple linear regression.\nTake residuals between actual values and \\(Z_1\\).\nCompute the second direction \\(Z_2\\) for the residuals with least squares.\n\n\nWhile PLS can reduce bias, it has the potential to increase variance. So it often performs no better than ridge regression or PCR."
  },
  {
    "objectID": "6.model-selection.html#regression-in-high-dimensions",
    "href": "6.model-selection.html#regression-in-high-dimensions",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Regression in High Dimensions",
    "text": "Regression in High Dimensions\nThe model with more predictors than the number of observations fits data exactly, which often leads to overfitting.\nHere are the workarounds.\n\nforward stepwise selection\nridge regression\nthe lasso\nprincipal components regression\n\nAlso, pay attention to the interpretation. If the variables entail multicollinearity, the values of coefficients are uninterpretable."
  },
  {
    "objectID": "6.model-selection.html#lab",
    "href": "6.model-selection.html#lab",
    "title": "Chapter 6: Linear Model Selection and Regularization",
    "section": "Lab",
    "text": "Lab\n\nlibrary(ISLR2)\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\n\nHitters = na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\n\n\nBest subset selection\n\nlibrary(leaps)\nregfit.full = regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\n\nregfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nreg.summary = summary(regfit.full)\n\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\npar(mfrow = c(2,2))\nplot(reg.summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\n\nwhich.max(reg.summary$adjr2)\n\n[1] 11\n\n\n\nplot(reg.summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\npoints(11, reg.summary$adjr2[11], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(reg.summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\npoints(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(reg.summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\npoints(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\nForward and Backward Stepwise Selection\n\nregfit.fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\n\nregfit.bwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\n\n\nCross-validation\n\nset.seed(1)\ntrain = sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)\ntest = (!train)\n\n\nregfit.best = regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)\n\n\ntest.mat = model.matrix(Salary ~ ., data = Hitters[test, ])\n\n\nval.errors = rep(NA, 19)\nfor(i in 1:19){\n  coefi = coef(regfit.best, id = i)\n  pred = test.mat[, names(coefi)] %*% coefi\n  val.errors[i] = mean((Hitters$Salary[test] - pred)^2)\n}\n\n\ncoef(regfit.best, which.min(val.errors))\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\n\npredict.regsubsets = function(object, newdata, id){\n  form = as.formula(object$call[[2]])\n  mat = model.matrix(form, newdata)\n  coefi = coef(object, id = id)\n  xvars = names(coefi)\n  mat[, xvars] %*% coefi\n}\n\n\nregfit.best = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\n\nk = 10\nn = nrow(Hitters)\nset.seed(1)\nfolds = sample(rep(1:k, length = n))\ncv.errors = matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))\n\n\nfor(j in 1:k){\n  best.fit = regsubsets(Salary ~ ., data = Hitters[folds != j,], nvmax = 19)\n  for (i in 1:19){\n    pred = predict(best.fit, Hitters[folds ==j, ], id = i)\n    cv.errors[j, i] = mean((Hitters$Salary[folds == j] - pred)^2)\n  }\n}\n\n\nmean.cv.errors = apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\n\n\npar(mfrow = c(1,1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\nWe found a 10-variable model is the best through cross-validation.\n\n\nRidge Regression\n\nx = model.matrix(Salary ~ ., Hitters)[, -1]\ny = Hitters$Salary\n\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-4\n\ngrid = 10^seq(10, -2, length = 100)\nridge.mod = glmnet(x, y, alpha = 0, lambda = grid)\n\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\n\n\npredict(ridge.mod, s = 50, type = \"coefficients\") # lambda = 50\n\n20 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  4.876610e+01\nAtBat       -3.580999e-01\nHits         1.969359e+00\nHmRun       -1.278248e+00\nRuns         1.145892e+00\nRBI          8.038292e-01\nWalks        2.716186e+00\nYears       -6.218319e+00\nCAtBat       5.447837e-03\nCHits        1.064895e-01\nCHmRun       6.244860e-01\nCRuns        2.214985e-01\nCRBI         2.186914e-01\nCWalks      -1.500245e-01\nLeagueN      4.592589e+01\nDivisionW   -1.182011e+02\nPutOuts      2.502322e-01\nAssists      1.215665e-01\nErrors      -3.278600e+00\nNewLeagueN  -9.496680e+00\n\n\n\nset.seed(1)\ntrain = sample(1:nrow(x), nrow(x) / 2)\ntest = (-train)\ny.test = y[test]\n\n\nridge.mod = glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)\nridge.pred = predict(ridge.mod, s = 4, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\n\nset.seed(1)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 0)\nplot(cv.out)\n\n\n\n\n\nbestlam = cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\n\nridge.pred = predict(ridge.mod, s = bestlam, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\n\nout = glmnet(x, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\n\n\nThe Lasso\n\nlasso.mod = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\n\nset.seed(1)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\n\n\nbestlam = cv.out$lambda.min\nlasso.pred = predict(lasso.mod, s = bestlam, newx = x[test, ])\nmean((lasso.pred -y.test)^2)\n\n[1] 143673.6\n\n\n\nout = glmnet(x, y, alpha = 1, lambda = grid)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\n\n\n\nPCR regression\n\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nset.seed(2)\npcr.fit = pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = \"CV\") # ten-fold cross-validation\n\n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    351.9    353.2    355.0    352.8    348.4    343.6\nadjCV          452    351.6    352.7    354.4    352.1    347.6    342.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       345.5    347.7    349.6     351.4     352.1     353.5     358.2\nadjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.7     349.4     339.9     341.6     339.2     339.6\nadjCV     348.0     347.7     338.2     339.7     337.2     337.6\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n\n\nvalidationplot(pcr.fit, val.type = \"MSEP\") # cross-validation MSE to be plotted\n\n\n\n\n\nset.seed(1)\npcr.fit = pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n\n\npcr.pred = predict(pcr.fit, x[test, ], ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142811.8\n\n\n\npcr.fit = pcr(y ~ x, scale = TRUE, ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90\n\n\n\n\nPartial Least Squares\n\nset.seed(1)\npls.fit = plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = \"CV\")\nsummary(pls.fit)\n\nData:   X dimension: 131 19 \n    Y dimension: 131 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           428.3    325.5    329.9    328.8    339.0    338.9    340.1\nadjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       339.0    347.1    346.4     343.4     341.5     345.4     356.4\nadjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        348.4     349.1     350.0     344.2     344.5     345.0\nadjCV     344.2     345.0     345.9     340.4     340.6     341.1\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71\nSalary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         93.17     96.05     97.08     97.61     97.97     98.70     99.12\nSalary    55.95     56.12     56.47     56.68     57.37     57.76     58.08\n        16 comps  17 comps  18 comps  19 comps\nX          99.61     99.70     99.95    100.00\nSalary     58.17     58.49     58.56     58.62\n\n\n\nvalidationplot(pls.fit, val.type = \"MSEP\")\n\n\n\n\n\npls.pred = predict(pls.fit, x[test, ], ncomp = 1)\nmean((pls.pred - y.test)^2)\n\n[1] 151995.3\n\n\n\npls.fit = plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 1)\nsummary(pls.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: kernelpls\nNumber of components considered: 1\nTRAINING: % variance explained\n        1 comps\nX         38.08\nSalary    43.05"
  },
  {
    "objectID": "8.tree-based.html",
    "href": "8.tree-based.html",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "",
    "text": "The tree-based methods are useful for interpretation but not competitive in prediction."
  },
  {
    "objectID": "8.tree-based.html#the-basics-of-decision-trees",
    "href": "8.tree-based.html#the-basics-of-decision-trees",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "The basics of Decision Trees",
    "text": "The basics of Decision Trees\n\nRegression Trees\n\nDivide the predictor space into J distinct and non-overlapping regions, \\(R_1, R_2, ..., R_j\\).\nReturn the mean of the response values for the training observations in \\(R_j\\).;\n\nThe goal is to find boxes \\(R_1, R_2, ..., R_j\\) that minimize the following RSS.\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j}(y_i - \\hat{y}_{R_J})^2\n\\]"
  },
  {
    "objectID": "8.tree-based.html#regression-trees",
    "href": "8.tree-based.html#regression-trees",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\n\nDivide the predictor space into J distinct and non-overlapping regions, \\(R_1, R_2, ..., R_j\\).\nReturn the mean of the response values for the training observations in \\(R_j\\).;\n\nThe goal is to find boxes \\(R_1, R_2, ..., R_j\\) that minimize the following RSS.\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j}(y_i - \\hat{y}_{R_J})^2\n\\]\nIt is computationally infeasible to consider every possible partition. Therefore, we take a top-down, greedy algorithm, recursive binary splitting.\nGiven\n\n\n\nseek the value of j and sn that minimize the following equation.\n\n\n\nNext, we split one of these two regions in the same way.\n\nTree Pruning\nHowever, this method tends to cause overfitting. So we adopt tree pruning that incorporates splitting as long as the decrease in the RSS exceeds some threshold.\nTo avoid to discard a split that leas to a large reduction in RSS later on, grow a very large tree \\(T_0\\), and then prune it back in order. Here, since estimating the cross-validation error for every possible subtree would be too cumbersome, we select a small set of subtrees for consideration, called cost complexity pruning or weakest link pruning.\nFind a subtree \\(T \\in T_0\\) that minimizes the following function.\n\n\\(|T|\\) is the number of terminal nodes of the tree T. \\(\\alpha |T|\\) is a penalty for a larger tree.\nWe determine \\(\\alpha\\) by cross-validation."
  },
  {
    "objectID": "8.tree-based.html#classification-trees",
    "href": "8.tree-based.html#classification-trees",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Classification Trees",
    "text": "Classification Trees\nWe use one of the following three criteria instead of RSS for classification trees.\n\nClassification error rate\n\n\n\nThe Gini index\n\n\\(\\hat{P}_{mk}\\) represents the proportion of training observations in the mth region that are from the kth class.\nThe small Gini index indicates a node contains predominantly observations from a single class.\n\n\nEntropy\n\nHere is the graph of \\(-xlog(x)\\).\n\nThe Gini index and entropy are quite similar numerically.\nIf prediction accuracy is the goal, the classification error rate is preferable."
  },
  {
    "objectID": "8.tree-based.html#bagging",
    "href": "8.tree-based.html#bagging",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Bagging",
    "text": "Bagging\nBagging is the method with the following steps.\n\nBootstrap by taking repeated samples from the training dataset.\nTrain our method on the bth bootstrapped training set.\nAverage all the predictions.\n\nAveraging a set of observations reduces variance. Given \\(Z_1, …, Z_n\\), each with variance \\(\\sigma^2\\), the variance of \\(\\bar{Z}\\) is \\(\\sigma^2/n\\).\nOut-of-Bag error estimation is a very straightforward way to estimate test error without the need to perform cross-validation.\n\nPredict the response for Out-of-bag (OOB) observations not used to fit a given bagged tree.\nAverage the predictions and compute overall OOB MSE or classification error, which is valid estimates of the test error.\n\nBagging improves prediction accuracy at the expense of interpretability. Also, in bagging, many of the bagged trees tend to be highly correlated, leading a less reduction in variance than averaging many uncorrelated quantities.\nBagging can get caught in local optima."
  },
  {
    "objectID": "8.tree-based.html#random-forests",
    "href": "8.tree-based.html#random-forests",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Random Forests",
    "text": "Random Forests\nRandom forests forces each split to consider only a random subset of the predictors. (Typically, we choose \\(m = \\sqrt{p}\\).) This make the average of the resulting trees less variable and more reliable."
  },
  {
    "objectID": "8.tree-based.html#boosting",
    "href": "8.tree-based.html#boosting",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Boosting",
    "text": "Boosting\nIn boosting, the trees are grown sequentially.\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\).\nGiven the current model, fit a decision tree to the residuals from the model as \\(\\hat{f}^b(x)\\) with d splits.\nAdd this new decision tree into the fitted function to update the residuals. \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\).\nUpdate the residuals. \\(r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\).\nOutput the boosted model. \\(\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x)\\).\n\nBoosting can overfit if B is too large.\nTypical values of \\(\\lambda\\) is 0.01 or 0.001.\nThe number of splits often \\(d = 1\\)."
  },
  {
    "objectID": "8.tree-based.html#bayesian-additive-regression-trees-bart",
    "href": "8.tree-based.html#bayesian-additive-regression-trees-bart",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Bayesian Additive Regression Trees (BART)",
    "text": "Bayesian Additive Regression Trees (BART)\nDefine three variables.\n\nK: the number of trees\nB: the number of iterations\nL: the number of burn-in iterations\n\nHere is the algorithm:\n\n\\(\\hat{f}_1^1 (x) = \\hat{f}_2^1 (x) = ... = \\hat{f}_K^1 (x) = \\frac{1}{nK} \\sum_{i=1}^{n} y_i\\)\n\\(\\hat{f}^1(x) = \\sum_{k=1}^{K} \\hat{f}_k^1(x) = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\)\nFor \\(b = 2,…,B\\):\n\nFor \\(k = 1,2,…,K\\):\n\nFor \\(i = 1,…,n\\), compute the current partial residual:\n\n\\(r_i = y_i - \\sum_{k' < k} \\hat{f}_{k'}^b (x_i) - \\sum_{k' > k} \\hat{f}_{k'}^{b-1}(x_i)\\)\nFit a new tree, \\(\\hat{f}_k^b(x)\\) to \\(r_i\\).\n\n\nCompute \\(\\hat{f}^b(x) = \\sum_{k=1}{K} \\hat{f}_k^b (x)\\)\n\nCompute the mean after L burn-in samples:\n\n\\(\\hat{f}(x) = \\frac{1}{B-L} \\sum_{b=L+1}{B} \\hat{f}^b(x)\\)\n\n\nWe typically throw away the first few models since they tend not to provide very good results.\nWe typically choose large values for B and K, and a moderate value for L: e.g., K=200, B=1000, L=100."
  },
  {
    "objectID": "8.tree-based.html#lab",
    "href": "8.tree-based.html#lab",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Lab",
    "text": "Lab\n\nFitting Classification Trees\n\nlibrary(tree)\nlibrary(ISLR2)\nattach(Carseats)\nHigh = factor(ifelse(Sales <= 8, \"No\", \"Yes\"))\n\n\nCarseats = data.frame(Carseats, High)\n\n\ntree.carseats = tree(High ~ . - Sales, Carseats)\n\n\nsummary(tree.carseats)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n\n\n\nplot(tree.carseats)\ntext(tree.carseats, pretty=0)\n\n\n\n\n\ntree.carseats\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 541.500 No ( 0.59000 0.41000 )  \n    2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n      4) Price < 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n        8) Income < 57 10  12.220 No ( 0.70000 0.30000 )  \n         16) CompPrice < 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n         17) CompPrice > 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n        9) Income > 57 36  35.470 Yes ( 0.19444 0.80556 )  \n         18) Population < 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n         19) Population > 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n      5) Price > 92.5 269 299.800 No ( 0.75465 0.24535 )  \n       10) Advertising < 13.5 224 213.200 No ( 0.81696 0.18304 )  \n         20) CompPrice < 124.5 96  44.890 No ( 0.93750 0.06250 )  \n           40) Price < 106.5 38  33.150 No ( 0.84211 0.15789 )  \n             80) Population < 177 12  16.300 No ( 0.58333 0.41667 )  \n              160) Income < 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n              161) Income > 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n             81) Population > 177 26   8.477 No ( 0.96154 0.03846 ) *\n           41) Price > 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n         21) CompPrice > 124.5 128 150.200 No ( 0.72656 0.27344 )  \n           42) Price < 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n             84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n             85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n              170) Price < 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n              171) Price > 109.5 24  32.600 No ( 0.58333 0.41667 )  \n                342) Age < 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n                343) Age > 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n           43) Price > 122.5 77  55.540 No ( 0.88312 0.11688 )  \n             86) CompPrice < 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n             87) CompPrice > 147.5 19  25.010 No ( 0.63158 0.36842 )  \n              174) Price < 147 12  16.300 Yes ( 0.41667 0.58333 )  \n                348) CompPrice < 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n                349) CompPrice > 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n              175) Price > 147 7   0.000 No ( 1.00000 0.00000 ) *\n       11) Advertising > 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n         22) Age < 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n           44) CompPrice < 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n             88) Income < 100 9  12.370 No ( 0.55556 0.44444 ) *\n             89) Income > 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n           45) CompPrice > 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Age > 54.5 20  22.490 No ( 0.75000 0.25000 )  \n           46) CompPrice < 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n           47) CompPrice > 122.5 10  13.860 No ( 0.50000 0.50000 )  \n             94) Price < 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n             95) Price > 125 5   0.000 No ( 1.00000 0.00000 ) *\n    3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n      6) Price < 135 68  49.260 Yes ( 0.11765 0.88235 )  \n       12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n         24) Price < 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price > 109 9  11.460 No ( 0.66667 0.33333 ) *\n       13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n      7) Price > 135 17  22.070 No ( 0.64706 0.35294 )  \n       14) Income < 46 6   0.000 No ( 1.00000 0.00000 ) *\n       15) Income > 46 11  15.160 Yes ( 0.45455 0.54545 ) *\n\n\n\nset.seed(2)\ntrain = sample(1:nrow(Carseats), 200)\nCarseats.test = Carseats[-train, ]\nHigh.test = High[-train]\ntree.carseats = tree(High ~ . - Sales, Carseats, subset = train)\ntree.pred = predict(tree.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\n\n\n(104 + 50) / 200\n\n[1] 0.77\n\n\nThe function cv.tree() performs cross-validation to determine the optimal level of tree complexity. FUN=prune.misclass indicates that we want the classification error rate to guide the cross-validation and pruning process. (The default is deviance.)\n\nset.seed(7)\ncv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)\ncv.carseats\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nsize: the number of terminal nodes of each tree\ndev: the error rate (the number of cross-validation error)\nk: the value of the cost-complexity parameter (\\(\\alpha\\))\n\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n\n\n\n\nThe size of 9 has the lowest dev.\n\nprune.carseats = prune.misclass(tree.carseats, best = 9) # obtain the nine-node tree\nplot(prune.carseats) \ntext(prune.carseats, pretty = 0)\n\n\n\n\n\ntree.pred = predict(prune.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred No Yes\n      No  97  25\n      Yes 20  58\n\n\n\n(97 + 58) / 200\n\n[1] 0.775\n\n\nThe pruned tree produced a more interpretable tree and also slightly improved classification accuracy.\nA larger pruned tree has lower classification accuracy:\n\nprune.carseats = prune.misclass(tree.carseats, best = 14) # obtain the 14-node tree\nplot(prune.carseats) \ntext(prune.carseats, pretty = 0)\n\n\n\n\n\ntree.pred = predict(prune.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n\n         High.test\ntree.pred  No Yes\n      No  102  31\n      Yes  15  52\n\n\n\n(102 + 52) / 200\n\n[1] 0.77\n\n\n\n\nFitting Regression Trees\n\nset.seed (1)\ntrain <- sample (1: nrow(Boston), nrow(Boston) / 2)\ntree.boston <- tree(medv ~ ., Boston , subset = train)\nsummary(tree.boston)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n\n\n\nplot(tree.boston)\ntext(tree.boston , pretty = 0)\n\n\n\n\n\ncv.boston <- cv.tree(tree.boston)\nplot(cv.boston$size , cv.boston$dev, type = \"b\")\n\n\n\n\n\nprune.boston <- prune.tree(tree.boston , best = 5)\nplot(prune.boston)\ntext(prune.boston , pretty = 0)\n\n\n\n\n\nyhat <- predict(tree.boston , newdata = Boston[-train , ])\nboston.test <- Boston[-train, \"medv\"]\nplot(yhat , boston.test)\nabline (0, 1)\n\n\n\n\n\nmean (( yhat - boston.test)^2)\n\n[1] 35.28688\n\n\n\nrpart\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ntr1 = rpart(medv ~ ., data = Boston)\npar(mfrow=c(1,2))\nplot(tr1)\nrpart.plot(tr1)\n\n\n\n\ncross-validation for pruning.\nCP: scaled \\(\\alpha\\).\nInstead of \\(RSS(T) + \\alpha |T|\\), rpart uses \\(\\frac{RSS(T)}{RSS(root)} + CP \\cdot |T|\\).\n\nprintcp(tr1)\n\n\nRegression tree:\nrpart(formula = medv ~ ., data = Boston)\n\nVariables actually used in tree construction:\n[1] crim  dis   lstat rm   \n\nRoot node error: 42716/506 = 84.42\n\nn= 506 \n\n        CP nsplit rel error  xerror     xstd\n1 0.452744      0   1.00000 1.00324 0.082971\n2 0.171172      1   0.54726 0.64125 0.057921\n3 0.071658      2   0.37608 0.43009 0.047418\n4 0.036164      3   0.30443 0.33278 0.041968\n5 0.033369      4   0.26826 0.32456 0.042045\n6 0.026613      5   0.23489 0.31425 0.041927\n7 0.015851      6   0.20828 0.28420 0.039942\n8 0.010000      7   0.19243 0.27750 0.040652\n\n\n\ntr1$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.45274420      0 1.0000000 1.0032418 0.08297088\n2 0.17117244      1 0.5472558 0.6412460 0.05792114\n3 0.07165784      2 0.3760834 0.4300911 0.04741826\n4 0.03616428      3 0.3044255 0.3327752 0.04196756\n5 0.03336923      4 0.2682612 0.3245644 0.04204536\n6 0.02661300      5 0.2348920 0.3142473 0.04192687\n7 0.01585116      6 0.2082790 0.2841993 0.03994168\n8 0.01000000      7 0.1924279 0.2774985 0.04065244\n\n\n\ncbind(tr1$cptable[, 1], c(-diff(tr1$cptable[, 3]), 0))\n\n        [,1]       [,2]\n1 0.45274420 0.45274420\n2 0.17117244 0.17117244\n3 0.07165784 0.07165784\n4 0.03616428 0.03616428\n5 0.03336923 0.03336923\n6 0.02661300 0.02661300\n7 0.01585116 0.01585116\n8 0.01000000 0.00000000\n\n\n\nprune(tr1, cp=0.3) # only 1 split\n\nn= 506 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 506 42716.300 22.53281  \n  2) rm< 6.941 430 17317.320 19.93372 *\n  3) rm>=6.941 76  6059.419 37.23816 *\n\n\n\nprune(tr1, cp=0.1) # 2 splits\n\nn= 506 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 506 42716.300 22.53281  \n  2) rm< 6.941 430 17317.320 19.93372  \n    4) lstat>=14.4 175  3373.251 14.95600 *\n    5) lstat< 14.4 255  6632.217 23.34980 *\n  3) rm>=6.941 76  6059.419 37.23816 *\n\n\n\nplotcp(tr1)\n\n\n\n\n\n\n\nBagging and Random Forests\n\nlibrary(randomForest)\nset.seed (1)\nbag.boston <- randomForest(medv ~ ., data = Boston, subset = train , mtry = 12, importance = TRUE)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n\n\nmtry indicates the number of predictors to be considered for each split.\n\nyhat.bag <- predict(bag.boston , newdata = Boston[-train , ])\nplot(yhat.bag , boston.test)\nabline (0, 1)\n\n\n\n\n\nmean (( yhat.bag - boston.test)^2)\n\n[1] 23.41916\n\n\n\nbag.boston <- randomForest(medv ~ ., data = Boston ,\nsubset = train , mtry = 12, ntree = 25)\nyhat.bag <- predict(bag.boston , newdata = Boston[-train , ])\nmean (( yhat.bag - boston.test)^2)\n\n[1] 25.75055\n\n\nntree indicates the number of trees.\nTo conduct random forest, we set mtry less than the number of variables.\n\nset.seed (1)\nrf.boston <- randomForest(medv ~ ., data = Boston,\n                          subset = train , mtry = 6, importance = TRUE)\nyhat.rf <- predict(rf.boston, newdata = Boston[-train , ])\nmean (( yhat.rf - boston.test)^2)\n\n[1] 20.06644\n\n\nThe prediction and error rate returned by randomForest are calculated based on OOB.\n\nimportance(rf.boston)\n\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n\n\n\nvarImpPlot(rf.boston)\n\n\n\n\n%IncMSE indicates the increase in MSE for regression / error rate for classification, when a given variable is not available (or permuted). In this case, lstat and rm are by far the two more important variables.\n\n\nBoosting\n\nlibrary(gbm)\nset.seed (1)\nboost.boston <- gbm(medv ~ ., data = Boston[train , ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4)\n\nWe set distribution as \"gaussian\" for a regression problem and \"bernoulli\" for a binary classification problem.\ninteraction.depth limits the depth of each tree.\n\nsummary(boost.boston)\n\n\n\n\n            var     rel.inf\nrm           rm 44.48249588\nlstat     lstat 32.70281223\ncrim       crim  4.85109954\ndis         dis  4.48693083\nnox         nox  3.75222394\nage         age  3.19769210\nptratio ptratio  2.81354826\ntax         tax  1.54417603\nindus     indus  1.03384666\nrad         rad  0.87625748\nzn           zn  0.16220479\nchas       chas  0.09671228\n\n\nBelow show partial dependence plots, illustrating the marginal effect of the selected variables on the response after integrating out the other variables.\n\npar(mfrow = c(1,2))\nplot(boost.boston , i = \"rm\")\n\n\n\nplot(boost.boston , i = \"lstat\")\n\n\n\n\n\nyhat.boost <- predict(boost.boston,\n                      newdata = Boston[-train , ], n.trees = 5000)\nmean (( yhat.boost - boston.test)^2)\n\n[1] 18.39057\n\n\n\nboost.boston <- gbm(medv ~ ., data = Boston[train , ],\n                    distribution = \"gaussian\", n.trees = 5000,\n                    interaction.depth = 4, shrinkage = 0.2, verbose = F)\nyhat.boost <- predict(boost.boston ,\n                      newdata = Boston[-train , ], n.trees = 5000)\nmean (( yhat.boost - boston.test)^2)\n\n[1] 16.54778\n\n\nshrinkage is the value of the shrinkage parameter \\(\\lambda\\) with 0.001 as default. In this case, \\(\\lambda=0.2\\) produces a lower test MSE.\n\ngbm.perf(boost.boston)\n\nOOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds>1 when calling gbm usually results in improved predictive performance.\n\n\n\n\n\n[1] 100\nattr(,\"smoother\")\nCall:\nloess(formula = object$oobag.improve ~ x, enp.target = min(max(4, \n    length(x)/10), 50))\n\nNumber of Observations: 5000 \nEquivalent Number of Parameters: 39.99 \nResidual Standard Error: 0.3387 \n\n\n\n\nBayesian Additive Regression Trees\n\nlibrary(BART)\nx <- Boston[, 1:12]\ny <- Boston[, \"medv\"]\nxtrain <- x[train, ]\nytrain <- y[train]\nxtest <- x[-train, ]\nytest <- y[-train]\nset.seed (1)\nbartfit <- gbart(xtrain , ytrain , x.test = xtest)\n\n*****Calling gbart: type=1\n*****Data:\ndata:n,p,np: 253, 12, 253\ny1,yn: 0.213439, -5.486561\nx1,x[n*p]: 0.109590, 20.080000\nxp1,xp[np*p]: 0.027310, 7.880000\n*****Number of Trees: 200\n*****Number of Cut Points: 100 ... 100\n*****burn,nd,thin: 100,1000,1\n*****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.795495,3,3.71636,21.7866\n*****sigma: 4.367914\n*****w (weights): 1.000000 ... 1.000000\n*****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,12,0\n*****printevery: 100\n\nMCMC\ndone 0 (out of 1100)\ndone 100 (out of 1100)\ndone 200 (out of 1100)\ndone 300 (out of 1100)\ndone 400 (out of 1100)\ndone 500 (out of 1100)\ndone 600 (out of 1100)\ndone 700 (out of 1100)\ndone 800 (out of 1100)\ndone 900 (out of 1100)\ndone 1000 (out of 1100)\ntime: 3s\ntrcnt,tecnt: 1000,1000\n\n\n\nyhat.bart <- bartfit$yhat.test.mean\nmean (( ytest - yhat.bart)^2)\n\n[1] 15.94718\n\n\nWe can check how many times each variable appeared in the collection of trees.\n\nord <- order(bartfit$varcount.mean , decreasing = T)\nbartfit$varcount.mean[ord]\n\n    nox   lstat     tax     rad      rm   indus    chas ptratio     age      zn \n 22.952  21.329  21.250  20.781  19.890  19.825  19.051  18.976  18.274  15.952 \n    dis    crim \n 14.457  11.007"
  },
  {
    "objectID": "8.tree-based.html#boosting-gradient-boosting-machine-gbm",
    "href": "8.tree-based.html#boosting-gradient-boosting-machine-gbm",
    "title": "Chapter 8: Tree-Based Methods",
    "section": "Boosting (Gradient Boosting Machine: GBM)",
    "text": "Boosting (Gradient Boosting Machine: GBM)\nIn boosting, the trees are grown sequentially.\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\).\nGiven the current model, fit a decision tree to the residuals from the model as \\(\\hat{f}^b(x)\\) with d splits.\nAdd this new decision tree into the fitted function to update the residuals. \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\).\nUpdate the residuals. \\(r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\).\nOutput the boosted model. \\(\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x)\\).\n\nBoosting can overfit if B is too large.\nTypical values of \\(\\lambda\\) is 0.01 or 0.001.\nThe number of splits often \\(d = 1\\).\n\nRandom Forest vs. Boosting\nRandom Forest has less number of tuning parameters. With proper tuning, boosting can perform better than random forest."
  },
  {
    "objectID": "7.beyond-linearity.html",
    "href": "7.beyond-linearity.html",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "",
    "text": "Polynomial regression\nStep functions\nRegression splines\nSmoothing splines\nLocal regression\nGeneralized additive models"
  },
  {
    "objectID": "7.beyond-linearity.html#basis-functions",
    "href": "7.beyond-linearity.html#basis-functions",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Basis Functions",
    "text": "Basis Functions\nPolynomial and piecewise-constant regression models are in fact special cases of a basis function approach, a family of functions or transformations that can be applied to a variable.\n\nHence, we can use least squares."
  },
  {
    "objectID": "7.beyond-linearity.html#polynomial-regression",
    "href": "7.beyond-linearity.html#polynomial-regression",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression"
  },
  {
    "objectID": "7.beyond-linearity.html#step-functions",
    "href": "7.beyond-linearity.html#step-functions",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Step Functions",
    "text": "Step Functions\nBreak the range of X into bins to convert a continuous variable into an ordered categorical variable."
  },
  {
    "objectID": "7.beyond-linearity.html#regression-splines",
    "href": "7.beyond-linearity.html#regression-splines",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nPiecewise Polynomials\n\nThe problem is the function is discontinuous around knots.\n\n\nConstraints and Splines\nConstraints:\n\nthe fitted curve must be continuous.\nBoth the first and second derivatives of the piecewise polynomials are continuous at knots ( except a linear spline).\n\n\n\nThe Spline Basis Representation\n\nE.g., for a cubic polynomial, add one truncated power basis function per knot as follows.\n\nwhere \\(\\xi\\) is the knot.\nThen, the first and second derivatives at each of the knots will remain continuous.\nThe number of predictors is $3+K$, i.e., \\(X, X^2, X^3, h(X,\\xi_1), h(X,\\xi+2),...,h(X,\\xi_K)\\).\nWith the intercept in addition to these predictors, the number of coefficients is \\(K+4\\), i.e., \\(K+4\\) degrees of freedom.\nSplines can have high variance at the outer range of the predictors.\n\n\nChoosing the Number and Locations of the Knots\nIn practice, it is common to place knots in a uniform fashion.\nTo determine the number of knots or degrees of freedom, cross-validation works well.\n\n\nComparison to Polynomial Regression\nRegression splines often give superior results to polynomial regression. Regression splines, which increase the number of knots but keep the degree fixed, produce more stable estimates."
  },
  {
    "objectID": "7.beyond-linearity.html#smoothing-splines",
    "href": "7.beyond-linearity.html#smoothing-splines",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\nThe function g that minimizes the following value is called smoothing splines, which makes RSS small but also the regression line smooth.\n\nwhere \\(\\lambda\\) is nonnegative.\nIf \\(\\lambda \\rightarrow \\infty\\), the regression line becomes a straight line.\nSmoothing splines is a shrunken version of a natural cubic spline with the region outside of the extreme knots perfectly linear.\n\nChoosing the Smoothing Parameter \\(\\lambda\\)\nDegrees of freedom: the number of free parameters\nEffective degrees of freedom \\(df_\\lambda\\): a measure of the flexibility of the smoothing spline.\n\\[\n\\hat{g}_\\lambda = S_\\lambda y\n\\]\n\\(\\hat{g}_\\lambda\\) indicates the fitted value of the smoothing spline.\n\\(S_\\lambda\\) is an n x n matrix.\nThen, the effective degrees of freedom is defined as\n\\[\ndf_\\lambda = \\sum_{i=1}^{n} \\{S_\\lambda\\}_{ii}\n\\]\nTo choose \\(\\lambda\\), cross-validation works. Especially, LOOCV can be computed very efficiently for smoothing splines.\n\n\\(\\hat{g}_\\lambda ^{(-i)}(x_i)\\) indicates the fitted value for the smoothing spline evaluated at \\(x_i\\), where the fit uses all the training data except for the ith observation."
  },
  {
    "objectID": "7.beyond-linearity.html#local-regression",
    "href": "7.beyond-linearity.html#local-regression",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Local Regression",
    "text": "Local Regression\nFit a target point \\(x_0\\) using only the nearby training observations.\n\nGather the fraction \\(s=k/n\\) of training points whose \\(x_i\\) are closest to \\(x_0\\).\nAssign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point, so that the point furthest from \\(x_0\\) has weight zero and the closest has the highest weight.\nFit a weighted least squares regression:\n\n\n\nThe fitted value at \\(x_0\\) is give by \\(\\hat{f}(x0) = \\hat{\\beta_0} + \\hat{\\beta_1} x_0\\).\n\nThe span s plays a significant role to determine smoothness. The larger s will lead to a global filt to the data using all of the training data.\nTo choose s, cross-validation works."
  },
  {
    "objectID": "7.beyond-linearity.html#generalized-additive-models-gam",
    "href": "7.beyond-linearity.html#generalized-additive-models-gam",
    "title": "Chapter 7: Moving Beyond Linearity",
    "section": "Generalized Additive Models (GAM)",
    "text": "Generalized Additive Models (GAM)\nQuantitive Regression\n\nQualitative Classification"
  }
]