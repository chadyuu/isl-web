---
title: "Chapter 8: Tree-Based Methods"
execute:
  cache: true
---

The tree-based methods are **useful for interpretation** but **not competitive in prediction**.

-   bagging
-   random forests
-   boosting
-   Bayesian additive regression trees

## Regression Trees

1.  Divide the predictor space into J distinct and non-overlapping regions, $R_1, R_2, ..., R_j$.
2.  Return the mean of the response values for the training observations in $R_j$.;

The goal is to find boxes $R_1, R_2, ..., R_j$ that minimize the following RSS.

$$
\sum_{j=1}^{J} \sum_{i \in R_j}(y_i - \hat{y}_{R_J})^2
$$

It is computationally infeasible to consider every possible partition. Therefore, we take a top-down, greedy algorithm, recursive binary splitting.

Given

![seek the value of j and sn that minimize the following equation.](images/paste-266A1F39.png)

![](images/paste-87B9C7E7.png)

Next, we split one of these two regions in the same way.

### Tree Pruning

However, this method tends to cause overfitting. So we adopt tree pruning that incorporates splitting as long as the decrease in the RSS exceeds some threshold.

To avoid to discard a split that leas to a large reduction in RSS later on, grow a very large tree $T_0$, and then prune it back in order. Here, since estimating the cross-validation error for every possible subtree would be too cumbersome, we select a small set of subtrees for consideration, called **cost complexity pruning** or **weakest link pruning**.

Find a subtree $T \in T_0$ that minimizes the following function.

![](images/paste-AE43BF20.png)

$|T|$ is the number of terminal nodes of the tree T. $\alpha |T|$ is a penalty for a larger tree.

We determine $\alpha$ by cross-validation.

## Classification Trees

We use one of the following three criteria instead of RSS for classification trees.

### Classification error rate

![](images/paste-C9CCBAED.png)

### The Gini index

![](images/paste-4B1DAF2F.png)

$\hat{P}_{mk}$ represents the proportion of training observations in the mth region that are from the kth class.

The small Gini index indicates a node contains predominantly observations from a single class.

### Entropy

![](images/paste-16170E9A.png)

Here is the graph of $-xlog(x)$.

![](images/paste-6C644DBE.png)

The Gini index and entropy are quite similar numerically.

If prediction accuracy is the goal, the classification error rate is preferable.

## Bagging

Bagging is the method with the following steps.

-   Bootstrap by taking repeated samples from the training dataset.

-   Train our method on the bth bootstrapped training set.

-   Average all the predictions.

Averaging a set of observations reduces variance. Given $Z_1, …, Z_n$, each with variance $\sigma^2$, the variance of $\bar{Z}$ is $\sigma^2/n$.

Out-of-Bag error estimation is a very straightforward way to estimate test error without the need to perform cross-validation.

-   Predict the response for Out-of-bag (OOB) observations not used to fit a given bagged tree.

-   Average the predictions and compute overall OOB MSE or classification error, which is valid estimates of the test error.

Bagging improves prediction accuracy at the expense of interpretability. Also, in bagging, many of the bagged trees tend to be highly correlated, leading a less reduction in variance than averaging many uncorrelated quantities.

Bagging can get caught in local optima.

## Random Forests

Random forests forces each split to consider only a random subset of the predictors. (Typically, we choose $m = \sqrt{p}$.) This make the average of the resulting trees less variable and more reliable.

## Boosting (Gradient Boosting Machine: GBM)

In boosting, the trees are grown sequentially.

-   Set $\hat{f}(x) = 0$ and $r_i = y_i$.

-   Given the current model, fit a decision tree to the residuals from the model as $\hat{f}^b(x)$ with d splits.

-   Add this new decision tree into the fitted function to update the residuals. $\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$.

-   Update the residuals. $r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)$.

-   Output the boosted model. $\hat{f}(x) = \sum_{b=1}^{B} \lambda \hat{f}^b(x)$.

Boosting can overfit if B is too large.

Typical values of $\lambda$ is 0.01 or 0.001.

The number of splits often $d = 1$.

### Random Forest vs. Boosting

Random Forest has less number of tuning parameters. With proper tuning, boosting can perform better than random forest.

## Bayesian Additive Regression Trees (BART)

Define three variables.

-   K: the number of trees

-   B: the number of iterations

-   L: the number of burn-in iterations

Here is the algorithm:

1.  $\hat{f}_1^1 (x) = \hat{f}_2^1 (x) = ... = \hat{f}_K^1 (x) = \frac{1}{nK} \sum_{i=1}^{n} y_i$
2.  $\hat{f}^1(x) = \sum_{k=1}^{K} \hat{f}_k^1(x) = \frac{1}{n} \sum_{i=1}^{n} y_i$
3.  For $b = 2,…,B$:
    1.  For $k = 1,2,…,K$:

        1.  For $i = 1,…,n$, compute the current partial residual:

            1.  $r_i = y_i - \sum_{k' < k} \hat{f}_{k'}^b (x_i) - \sum_{k' > k} \hat{f}_{k'}^{b-1}(x_i)$

            2.  Fit a new tree, $\hat{f}_k^b(x)$ to $r_i$.

    2.  Compute $\hat{f}^b(x) = \sum_{k=1}{K} \hat{f}_k^b (x)$
4.  Compute the mean after L burn-in samples:
    1.  $\hat{f}(x) = \frac{1}{B-L} \sum_{b=L+1}{B} \hat{f}^b(x)$

We typically throw away the first few models since they tend not to provide very good results.

We typically choose large values for B and K, and a moderate value for L: e.g., K=200, B=1000, L=100.

## Lab

### Fitting Classification Trees

```{r}
library(tree)
library(ISLR2)
attach(Carseats)
High = factor(ifelse(Sales <= 8, "No", "Yes"))
```

```{r}
Carseats = data.frame(Carseats, High)
```

```{r}
tree.carseats = tree(High ~ . - Sales, Carseats)
```

```{r}
summary(tree.carseats)
```

```{r}
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

```{r}
tree.carseats
```

```{r}
set.seed(2)
train = sample(1:nrow(Carseats), 200)
Carseats.test = Carseats[-train, ]
High.test = High[-train]
tree.carseats = tree(High ~ . - Sales, Carseats, subset = train)
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

```{r}
(104 + 50) / 200
```

The function `cv.tree()` performs cross-validation to determine the optimal level of tree complexity. `FUN=prune.misclass` indicates that we want the classification error rate to guide the cross-validation and pruning process. (The default is deviance.)

```{r}
set.seed(7)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
```

`size`: the number of terminal nodes of each tree

`dev`: the error rate (the number of cross-validation error)

`k`: the value of the cost-complexity parameter ($\alpha$)

```{r}
par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

The size of 9 has the lowest `dev`.

```{r}
prune.carseats = prune.misclass(tree.carseats, best = 9) # obtain the nine-node tree
plot(prune.carseats) 
text(prune.carseats, pretty = 0)
```

```{r}
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

```{r}
(97 + 58) / 200
```

The pruned tree produced a more interpretable tree and also slightly improved classification accuracy.

A larger pruned tree has lower classification accuracy:

```{r}
prune.carseats = prune.misclass(tree.carseats, best = 14) # obtain the 14-node tree
plot(prune.carseats) 
text(prune.carseats, pretty = 0)
```

```{r}
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

```{r}
(102 + 52) / 200
```

### Fitting Regression Trees

```{r}
set.seed (1)
train <- sample (1: nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston , subset = train)
summary(tree.boston)
```

```{r}
plot(tree.boston)
text(tree.boston , pretty = 0)
```

```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size , cv.boston$dev, type = "b")
```

```{r}
prune.boston <- prune.tree(tree.boston , best = 5)
plot(prune.boston)
text(prune.boston , pretty = 0)
```

```{r}
yhat <- predict(tree.boston , newdata = Boston[-train , ])
boston.test <- Boston[-train, "medv"]
plot(yhat , boston.test)
abline (0, 1)
```

```{r}
mean (( yhat - boston.test)^2)
```

#### rpart

```{r}
library(rpart)
library(rpart.plot)
set.seed(1234)
tr1 = rpart(medv ~ ., data = Boston)
par(mfrow=c(1,2))
plot(tr1)
rpart.plot(tr1)
```

cross-validation for pruning.

`CP`: scaled $\alpha$.

Instead of $RSS(T) + \alpha |T|$, `rpart` uses $\frac{RSS(T)}{RSS(root)} + CP \cdot |T|$.

```{r}
printcp(tr1)
```

```{r}
tr1$cptable
```

```{r}
cbind(tr1$cptable[, 1], c(-diff(tr1$cptable[, 3]), 0))
```

```{r}
prune(tr1, cp=0.3) # only 1 split
```

```{r}
prune(tr1, cp=0.1) # 2 splits
```

```{r}
plotcp(tr1)
```

### Bagging and Random Forests

```{r}
library(randomForest)
set.seed (1)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train , mtry = 12, importance = TRUE)
bag.boston
```

`mtry` indicates the number of predictors to be considered for each split.

```{r}
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
plot(yhat.bag , boston.test)
abline (0, 1)
```

```{r}
mean (( yhat.bag - boston.test)^2)
```

```{r}
bag.boston <- randomForest(medv ~ ., data = Boston ,
subset = train , mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
mean (( yhat.bag - boston.test)^2)
```

`ntree` indicates the number of trees.

To conduct random forest, we set `mtry` less than the number of variables.

```{r}
set.seed (1)
rf.boston <- randomForest(medv ~ ., data = Boston,
                          subset = train , mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train , ])
mean (( yhat.rf - boston.test)^2)
```

The prediction and error rate returned by randomForest are calculated based on OOB.

```{r}
importance(rf.boston)
```

```{r}
varImpPlot(rf.boston)
```

`%IncMSE` indicates the increase in MSE for regression / error rate for classification, when a given variable is not available (or permuted). In this case, `lstat` and `rm` are by far the two more important variables.

### Boosting

```{r}
library(gbm)
set.seed (1)
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4)
```

We set `distribution` as `"gaussian"` for a regression problem and `"bernoulli"` for a binary classification problem.

`interaction.depth` limits the depth of each tree.

```{r}
summary(boost.boston)
```

Below show partial dependence plots, illustrating the marginal effect of the selected variables on the response after integrating out the other variables.

```{r}
par(mfrow = c(1,2))
plot(boost.boston , i = "rm")
plot(boost.boston , i = "lstat")
```

```{r}
yhat.boost <- predict(boost.boston,
                      newdata = Boston[-train , ], n.trees = 5000)
mean (( yhat.boost - boston.test)^2)
```

```{r}
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston ,
                      newdata = Boston[-train , ], n.trees = 5000)
mean (( yhat.boost - boston.test)^2)
```

`shrinkage` is the value of the shrinkage parameter $\lambda$ with 0.001 as default. In this case, $\lambda=0.2$ produces a lower test MSE.

```{r}
gbm.perf(boost.boston)
```

Check the range of parameters.

```{r}
 gbm(medv ~ ., data = Boston[train , ],
  distribution = "gaussian", n.trees = 1,
  interaction.depth = 4, shrinkage = -1)
```

`n.trees` can be 1.

```{r}
 gbm(medv ~ ., data = Boston[train , ],
  distribution = "gaussian", n.trees = 1,
  interaction.depth = 4)
```

`bag.fraction` should be greater than 0.

```{r}
 gbm(medv ~ ., data = Boston[train , ],
  distribution = "gaussian", n.trees = 1,
  interaction.depth = 4, bag.fraction = 1)
```

```{r}
 gbm(medv ~ ., data = Boston[train , ],
  distribution = "gaussian", n.trees = 1,
  interaction.depth = 4, bag.fraction = 100)
```

### Bayesian Additive Regression Trees

```{r}
library(BART)
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed (1)
bartfit <- gbart(xtrain , ytrain , x.test = xtest)
```

```{r}
yhat.bart <- bartfit$yhat.test.mean
mean (( ytest - yhat.bart)^2)
```

We can check how many times each variable appeared in the collection of trees.

```{r}
ord <- order(bartfit$varcount.mean , decreasing = T)
bartfit$varcount.mean[ord]
```
