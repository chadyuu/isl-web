---
title: "Chapter 5: Resampling Methods"
execute:
  cache: true
---

# Resampling Methods

1.  cross-validation
2.  bootstrap

## Cross-validation

### The Validation Set Approach

Split data into the following two sets

-   a training set

-   a validation set (hold-out set)

We can estimate the test error rate from the validation set error rate, which can be highly variable.

### Leave-One-Out Cross-Validation (LOOCV)

LOOCV addresses the high variability of the validation set approach.

With least squares linear or polynomial regression, the cost of LOOCV is the same as that of a single model.

![](images/paste-E6965AC7.png)

### k-Fold Cross-Validation

It randomly divides the set of observations into k groups.

![](images/paste-3ACC668C.png)

LOOCV is the special case of k-fold CV in which k is set to equal n.

-   k-fold CV with k \< n has a computational advantage to LOOCV.

-   LOOCV reduces more bias than k-fold CV.

-   LOOCV has higher variance than k-fold CV.

    -   each of n models is trained on an almost identical set of observations.

-   k-fold CV often gives more accurate estimates of the test error rate than LOOCV.

So typically, **k = 5 or k = 10** tends to yield more accurate test error rate estimates.

### Cross-Validation on Classification Problems

![](images/paste-1F8C2F80.png)

$$Err_i = I(y_i \neq \hat{y_i})$$

## The Bootstrap

it is used to quantify the uncertainty associated with a given estimator.

E.g., the bootstrap can be used to estimate the standard errors of the coefficients.

![](images/paste-689277A6.png)

## Lab

### The Validation Set Approach

```{r}
library(ISLR2)
set.seed(1)
train = sample(392, 196)
```

Simple Linear Regression Model:

```{r}
lm.fit = lm(mpg ~ horsepower, data = Auto, subset = train)
```

```{r}
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2) # the estimated test MSE
```

polynomial regression models:

```{r}
lm.fit2 = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2) # the estimated test MSE
```

```{r}
lm.fit3 = lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2) # the estimated test MSE
```

We prefer the quadratic regression model based on the validation MSE.

### LOOCV

```{r}
glm.fit = glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
```

```{r}
library(boot)
cv.err = cv.glm(Auto, glm.fit)
cv.err$delta # [1]: the standard k-fold CV estimate, [2]: a bias-corrected version
```

```{r}
cv.error = rep(0, 10)
for(i in 1:10){
  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```

```{r}
plot(cv.error)
```

We see a sharp drop in the estimated test MSE between the linear and quadratic fits, while not from higher-order polynomials.

### k-Fold Cross Validation

```{r}
set.seed(17)
cv.error.10 = rep(0, 10)
for (i in 1:10) {
  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
```

```{r}
plot(cv.error.10)
```

Higher-order polynomials than quadratic does not show improvement of test MSE.

### The Bootstrap

```{r}
head(Portfolio)
```

```{r}
alpha.fn = function(data, index){
  X = data$X[index]
  Y = data$Y[index]
  (var(Y) - cov(X, Y) / (var(X) + var(Y) - 2 * cov(X, Y)))
}
```

```{r}
alpha.fn(Portfolio, 1:100)
```

```{r}
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
```

```{r}
boot(Portfolio, alpha.fn, R = 1000)
```

```{r}
boot.fn = function(data, index){
  coef(lm(mpg ~ horsepower, data= data, subset = index))
}
```

```{r}
boot.fn(Auto, 1:392)
```

```{r}
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
```

```{r}
boot(Auto, boot.fn, 1000)
```

```{r}
summary(lm(mpg ~ horsepower , data = Auto))$coef
```

```{r}
boot.fn2 = function(data, index){
  coef(lm(mpg ~ horsepower + I(horsepower^2), data= data, subset = index))
}
```

```{r}
set.seed(1)
boot(Auto, boot.fn2, 1000)
```

```{r}
summary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))
```
